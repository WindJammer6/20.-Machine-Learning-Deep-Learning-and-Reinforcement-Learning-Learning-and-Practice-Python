{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49488806",
   "metadata": {},
   "source": [
    "## Demonstrating how the mathematics of the Activation Functions look like\n",
    "However, do note that the codes here are all purely for demonstration purposes, for your understanding. It is very unlikely that you will need to implement these Activation Functions yourself when building Neural Networks, since DL libraries like PyTorch, TensorFlow and Keras, which you will most likely use when building Neural Networks, has already integrated all these in their ready-made API functions.\n",
    "\n",
    "We'll be going through 4 types of the most common Activation Functions used in Neural Networks:\n",
    "- Sigmoid Function\n",
    "- Tanh Function\n",
    "- ReLU Function\n",
    "- Leaky ReLU Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2662c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd756a4",
   "metadata": {},
   "source": [
    "### Sigmoid Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5c64ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.7310585786300049\n",
      "4.780892883885469e-25\n",
      "0.6224593312018546\n"
     ]
    }
   ],
   "source": [
    "def sigmoid_activation_function(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "print(sigmoid_activation_function(100))\n",
    "print(sigmoid_activation_function(1))\n",
    "print(sigmoid_activation_function(-56))\n",
    "print(sigmoid_activation_function(0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42a2e89",
   "metadata": {},
   "source": [
    "### Tanh Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fea757c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0\n",
      "1.0\n",
      "0.7615941559557649\n"
     ]
    }
   ],
   "source": [
    "def tanh_activation_function(x):\n",
    "    return (math.exp(x) - math.exp(-x)) / (math.exp(x) + math.exp(-x))\n",
    "\n",
    "print(tanh_activation_function(-56))\n",
    "print(tanh_activation_function(50))\n",
    "print(tanh_activation_function(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8830644c",
   "metadata": {},
   "source": [
    "### ReLU Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cb6a400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "def relu_activation_function(x):\n",
    "    return max(0,x)\n",
    "\n",
    "print(relu_activation_function(-100))\n",
    "print(relu_activation_function(8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9012c2f",
   "metadata": {},
   "source": [
    "### Leaky ReLU Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e376927d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-10.0\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "def leaky_relu_activation_function(x):\n",
    "    return max(0.1 * x,x)\n",
    "\n",
    "print(leaky_relu_activation_function(-100))\n",
    "print(leaky_relu_activation_function(8))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
