{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07f37ed0",
   "metadata": {},
   "source": [
    "# Reinforcement Learning (RL) with Gymnasium and Stable Baselines3 Tutorial (Part 1)\n",
    "Source: \n",
    "- https://www.youtube.com/watch?v=Mut_u40Sqz4&t=6144s (Nicholas Renotte) (YouTube video by Nicholas Renotte titled, \n",
    "'Reinforcement Learning in 3 Hours | Full Course using Python')\n",
    "\n",
    "Documentations:\n",
    "- Gymnasium: https://gymnasium.farama.org/ (This library provides standardized environments for developing and testing RL algorithms)\n",
    "- Stable Baselines3: https://stable-baselines3.readthedocs.io/en/master/guide/quickstart.html (This library provides a suite of pre-implemented RL algorithms based on PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12377cc4",
   "metadata": {},
   "source": [
    "## How is an RL Environment defined?\n",
    "An RL Environment is typically modeled as the 5-tuple:\n",
    "```text\n",
    "ùëÄ=(ùëÜ,ùê¥,ùëÉ,ùëÖ,ùõæ)\n",
    "```\n",
    "\n",
    "An RL Environment is defined as 5-tuple in the framework of a Markov Decision Process (MDP):\n",
    "\n",
    "| Symbol              | Name                       | Description                                                                               |\n",
    "| ------------------- | -------------------------- | ----------------------------------------------------------------------------------------- |\n",
    "| $S$                 | **States**                 | The set of all possible states the agent can be in                                        |\n",
    "| $A$                 | **Actions**                | The set of all possible actions the agent can take                                        |\n",
    "| $P(s' \\mid s, a)$   | **Transition Probability** | The probability of moving to state $s'$ after taking action $a$ in state $s$              |\n",
    "| $R(s, a)$           | **Reward Function**        | The expected reward received after taking action $a$ in state $s$                         |\n",
    "| $\\gamma \\in [0, 1]$ | **Discount Factor**        | The factor by which future rewards are discounted (controls how far-sighted the agent is) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e57f69",
   "metadata": {},
   "source": [
    "## How does Gymnasium represent each of these components of the RL Environment?\n",
    "**States**/**Observations** and  **Actions**  \n",
    "- Box ‚Äì n-dimensional tensor, range of values (continuous values)\n",
    "    ```\n",
    "    E.g. Box(0, 1, shape=(3,3))\n",
    "    ```\n",
    "- Discrete ‚Äì Set of items (discrete values)\n",
    "    ```\n",
    "    E.g. Discrete(3)\n",
    "    ```\n",
    "- Tuple ‚Äì Tuple of other spaces (e.g., Box or Discrete)\n",
    "    ```\n",
    "    E.g. Tuple((Discrete(2), Box(0, 100, shape=(1,))))\n",
    "    ```\n",
    "- Dict ‚Äì Dictionary of spaces (e.g., Box or Discrete)\n",
    "    ```\n",
    "    E.g. Dict({\"height\": Discrete(2), \"speed\": Box(0, 100, shape=(1,))})\n",
    "    ```\n",
    "- MultiBinary ‚Äì One-hot encoded binary values\n",
    "    ```\n",
    "    E.g. MultiBinary(4)\n",
    "    ```\n",
    "- MultiDiscrete ‚Äì Multiple discrete values\n",
    "    ```\n",
    "    E.g. MultiDiscrete([5, 2, 2])\n",
    "    ```\n",
    "\n",
    "**Transition Probability**  \n",
    "- abstracted out by the Gymmnasium library\n",
    "\n",
    "**Reward Function**  \n",
    "- abstracted out by the Gymmnasium library\n",
    "\n",
    "**Discount Factor**\n",
    "- abstracted out by the Gymmnasium library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d803b1e",
   "metadata": {},
   "source": [
    "### What is the difference between States and Observations?\n",
    "RL agents only act on observations, not states. Optimal behavior of RL agents assumes knowledge of the underlying state (or estimates of it).\n",
    "\n",
    "| **Aspect**          | **State**                                                    | **Observation**                                                 |\n",
    "| ------------------- | ------------------------------------------------------------ | --------------------------------------------------------------- |\n",
    "| **Definition**      | The **true internal configuration** of the environment       | The **information** the agent **receives** from the environment |\n",
    "| **Completeness**    | Often assumed to be **complete** (Markov property holds)     | May be **partial**, noisy, or incomplete view of the state      |\n",
    "| **Markov Property** | A true state satisfies: future depends only on current state | Observations may not satisfy the Markov property                |\n",
    "| **Agent‚Äôs View**    | Agent may not have access to the full state                  | Agent always uses observations to decide actions                |\n",
    "| **Example**         | All object positions, velocities, and environment internals  | Camera image, radar scan, or any sensor reading                 |\n",
    "\n",
    "**MDP vs POMDP**\n",
    "- In fully observable environments (e.g., many standard RL benchmarks), the observation is equivalent to the state. This is assumed in Markov Decision Processes (MDPs).\n",
    "- In Partially Observable MDPs (POMDPs), the agent sees only observations and must infer the state using memory or belief models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46deb6f",
   "metadata": {},
   "source": [
    "## 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77af2fc7",
   "metadata": {},
   "source": [
    "**To run Gymnasium and Stable Baselines3 libraries, it is HIGHLY recommended to create a virtual environment and download the dependencies/requirements in the virtual environment seperately to prevent conflicts in libraries!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d327991e",
   "metadata": {},
   "source": [
    "### How to set up a virtual environment in VS Code?\n",
    "1. **Create a virtual environment**\n",
    "    ```bash\n",
    "    python -m venv venv\n",
    "    ```\n",
    "    This creates a folder named venv/ containing the isolated environment.\n",
    "\n",
    "2. **Activate the virtual environment**\n",
    "\n",
    "    For Windows:\n",
    "    ```bash\n",
    "    .\\venv\\Scripts\\activate\n",
    "    ```\n",
    "    For macOS/Linux:\n",
    "    ```bash\n",
    "    source venv/bin/activate\n",
    "    ```\n",
    "    You‚Äôll know it‚Äôs activated when your terminal prompt changes to show (venv).\n",
    "\n",
    "3. **Now you can install dependencies inside the virtual environment!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26538f5a",
   "metadata": {},
   "source": [
    "### What dependencies/requirements to download? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649db53c",
   "metadata": {},
   "source": [
    "**For Gymnasium library**\n",
    "```bash\n",
    "pip install gymnasium\n",
    "```\n",
    "\n",
    "**For Stable Baselines3 library**\n",
    "```bash\n",
    "pip install stable-baselines3[extra]\n",
    "```\n",
    "\n",
    "Source(s):\n",
    "- https://github.com/AndreM96/Stable_Baseline3_Gymnasium_Tutorial (AndreM96 on Github)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7bbf80",
   "metadata": {},
   "source": [
    "Just for demonstration purposes, the RL algorithm that we will be using here is the Proximal Policy Optimization (PPO) DRL algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80b9b41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a841db34",
   "metadata": {},
   "source": [
    "## 2. Load RL Environment and testing if it works with a baseline algorithm that takes random actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f13962",
   "metadata": {},
   "source": [
    "Just for demonstration purposes, the RL Environment that we will be using here is the \"CartPole-v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bccdc18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State: (array([ 0.0164126 ,  0.02874528, -0.01213397, -0.00579226], dtype=float32), {})\n",
      "Episode: 0 Score: 30.0\n",
      "Initial State: (array([ 0.00957305,  0.00781111, -0.02901817, -0.03707879], dtype=float32), {})\n",
      "Episode: 1 Score: 30.0\n",
      "Initial State: (array([ 0.0052368 ,  0.03448069, -0.00121296, -0.03387132], dtype=float32), {})\n",
      "Episode: 2 Score: 12.0\n",
      "Initial State: (array([ 0.00475164, -0.01795405,  0.0135004 , -0.03343001], dtype=float32), {})\n",
      "Episode: 3 Score: 12.0\n",
      "Initial State: (array([-0.00023687,  0.01651358, -0.01636876,  0.00142939], dtype=float32), {})\n",
      "Episode: 4 Score: 25.0\n",
      "Initial State: (array([0.01125989, 0.01252995, 0.01655364, 0.00256611], dtype=float32), {})\n",
      "Episode: 5 Score: 19.0\n"
     ]
    }
   ],
   "source": [
    "environment_name = \"CartPole-v1\"\n",
    "env = gym.make(environment_name, render_mode=\"human\")\n",
    "\n",
    "episodes = 5\n",
    "for episode in range(0, episodes+1):\n",
    "    # Initialise starting state of the RL agent in the RL Environment before an episode, done to false, and starting \n",
    "    # episode score to 0\n",
    "    state = env.reset()\n",
    "    print(f\"Initial State: {state}\")\n",
    "    done = False\n",
    "    episode_score = 0\n",
    "\n",
    "    # During an episode:\n",
    "    while not done:\n",
    "        env.render()\n",
    "        # RL agent determines action to take\n",
    "        # - In this case, we are randomly sampling an action to take by our RL agent in the RL Environment (this line of\n",
    "        #   code defines that baseline algorithm that takes random actions (instead of an RL algorithm))\n",
    "        action = env.action_space.sample()\n",
    "        # RL Environment generates the next state and reward gained upon taking the action in the current state\n",
    "        n_state, reward, done, truncated, info = env.step(action)\n",
    "        # Append the reward gained upon taking the action in the current state to the cumulative episode date\n",
    "        episode_score += reward\n",
    "\n",
    "    print(f\"Episode: {episode} Score: {episode_score}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d356cd6",
   "metadata": {},
   "source": [
    "### Understanding the RL Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d2c658",
   "metadata": {},
   "source": [
    "From the Gymansium's \"CartPole-v1\" RL Environment documentation: https://gymnasium.farama.org/environments/classic_control/cart_pole/\n",
    "\n",
    "**States**  \n",
    "Type: Box(4)\n",
    "| Num | Observation           | Min               | Max             |\n",
    "| --- | --------------------- | ----------------- | --------------- |\n",
    "| 0   | Cart Position         | -4.8              | 4.8             |\n",
    "| 1   | Cart Velocity         | -Inf              | Inf             |\n",
    "| 2   | Pole Angle            | -0.418 rad (-24¬∞) | 0.418 rad (24¬∞) |\n",
    "| 3   | Pole Angular Velocity | -Inf              | Inf             |\n",
    "\n",
    "**Actions**  \n",
    "Type: Discrete(2)\n",
    "| Num | Action                 |\n",
    "| --- | ---------------------- |\n",
    "| 0   | Push cart to the left  |\n",
    "| 1   | Push cart to the right |\n",
    "\n",
    "Note:\n",
    "The amount the velocity that is reduced or increased is not fixed; it depends on the angle the pole is pointing. This is because the center of gravity of the pole increases the amount of energy needed to move the cart underneath it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09aee170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n",
      "Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "# Understanding the state and action spaces used in the Gymnasium's \"CartPole-v1\" RL Environment\n",
    "print(env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f31e19a",
   "metadata": {},
   "source": [
    "## 3. Vectorise RL Environment and Train an PPO DRL algorithm in a RL Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3416be12",
   "metadata": {},
   "source": [
    "### What is an Reinforcement Learning (RL) algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e22779",
   "metadata": {},
   "source": [
    "An RL algorithm involves an agent performing actions in an RL environment, receiving rewards or penalties based on those actions, and adjusting its behavior accordingly. This loop helps the agent improve its decision-making over time to maximize the cumulative reward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc16bfc8",
   "metadata": {},
   "source": [
    "### How does a Reinforcement Learning (RL) algorithm 'learn'?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f55666",
   "metadata": {},
   "source": [
    "In ML and DL, we learnt that ML/DL algorithms 'learn' by updating the ML/DL algorithm's weights and biases as more datas are fed into the ML/DL algorithm, and after many iterations of training, it makes accurate predictions. \n",
    "\n",
    "**This is no different in RL.**\n",
    "\n",
    "In RL, the RL algorithms uses various architectures to 'learn' by updating the RL algorithm's weights and biases as it interacts more with the RL Environment (via the reward mechanism). The 'learning' architecture used also defines whether a RL algorithm is a **Classical RL algorithm** or a **Deep RL (DRL) algorithm**.\n",
    "\n",
    "**Classical RL algorithm learning architectures**  \n",
    "Uses tables or simple functions:\n",
    "| Type                          | Description                                                                      | Example             |\n",
    "| ----------------------------- | -------------------------------------------------------------------------------- | ------------------- |\n",
    "| **Tabular policy**            | Table stores the best action for each discrete state                             | `œÄ[s] = a`          |\n",
    "| **Tabular stochastic policy** | Table of probabilities for each action in each state                             | `œÄ[a][s] = P(a \\| s)` |\n",
    "| **Value-based methods**       | Use a value table (e.g., Q-table) and derive policy as `œÄ(s) = argmax Q(s,a)`    | Q-Learning          |\n",
    "| **Policy iteration**          | Alternates between evaluating a policy and improving it based on value estimates | Dynamic Programming |      |\n",
    "| **Function approximation**    | Uses linear models or tile coding to generalize across large state spaces        | `œÄ(s) = Œ∏·µÄœÜ(s)`     |\n",
    "\n",
    "**Deep RL (DRL) algorithm learning architectures**  \n",
    "Uses neural networks or its variants,\n",
    "- FNN/MLP\n",
    "- CNN\n",
    "- RNN\n",
    "- LSTM\n",
    "- GRU\n",
    "\n",
    "In RL, after many iterations of training, it makes accurate predictions, more specifically, it behaves better/takes better actions. \n",
    "\n",
    "These RL algorithm 'learning' architectures is also called **Policy**, which defines how the agent chooses actions based on its current state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d840f63c",
   "metadata": {},
   "source": [
    "### What does a Vectorised RL Environment mean?\n",
    "Vectorized RL Environments are RL Environments that can be made to run in parallel, allowing multiple simulations at once to increase training speed of the RL algorithm.\n",
    "\n",
    "A non-vectorized RL Environment does not allow for being made to run in parallel (only one simulation can run at a time).\n",
    "\n",
    "In Gymnasium, some RL Environments are vectorized by default (e.g. Breakout), while others are not (e.g. CartPole). But when training a RL algorithm from Stable Baselines3, it is required for the RL Environment to be vectorized as well (even if you dont intend to run them in parallel)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed840f57",
   "metadata": {},
   "source": [
    "Since the RL Environment used here is \"CartPole-v1\", which is not vectorized by default, you need to manually vectorize them, and you can do so as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee521b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_name = \"CartPole-v1\"\n",
    "env = gym.make(environment_name, render_mode=\"human\")\n",
    "\n",
    "# Since the RL Environment used here, \"CartPole-v1\", is non-vectorized, we make it vectorized by placing it in the \n",
    "# 'DummyVecEnv' object (how this is done is shown here), which acts as a sort of wrapper to convert the non-vectorized \n",
    "# RL Environment into a vectorized RL Environment\n",
    "env = DummyVecEnv([lambda: env])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0605d69b",
   "metadata": {},
   "source": [
    "### For logging purposes of the training process of the PPO DRL algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca901c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\\logs\n"
     ]
    }
   ],
   "source": [
    "# Stating the path where we want to store our training logs files in the local folder './Training_Tutorial/logs'\n",
    "log_path = os.path.join('Training_Tutorial', 'logs')\n",
    "print(log_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab83cefd",
   "metadata": {},
   "source": [
    "### Creating the PPO DRL algorithm in the RL Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbd9bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# What does each of the parameters in the 'PPO' DRL algorithm class mean?\n",
    "# - 'policy' (e.g. 'MlpPolicy'  - refers to the learning architecture used a the policy of the RL algorithm, which in this\n",
    "#               or 'CnnPolicy')   is FNN/MLP\n",
    "# - 'env'                       - refers to the RL environment to train the RL algorithm in\n",
    "# - 'verbose'                   - controls how much information is printed to the console/log during training\n",
    "#                                 -> 'verbose=0' means 'Silent', no output at all\n",
    "#                                 -> 'verbose=1' means 'Info', shows key training events: episode rewards, updates, losses, etc.\n",
    "#                                 -> 'verbose=2' means 'Debug' shows more detailed info like hyperparameters, rollout steps, and internal logs\n",
    "# - 'tensorboard_log'           - states to do the training logging in Tensorboard\n",
    "PPO_DRL_model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1e82ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mInit signature:\u001b[0m\n",
      "\u001b[0mPPO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mpolicy\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstable_baselines3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mActorCriticPolicy\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0menv\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgymnasium\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEnv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mForwardRef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'VecEnv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mlearning_rate\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0003\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mn_steps\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2048\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mn_epochs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mgamma\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.99\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mgae_lambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.95\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mclip_range\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mclip_range_vf\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mNoneType\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mnormalize_advantage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0ment_coef\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mvf_coef\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmax_grad_norm\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0muse_sde\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0msde_sample_freq\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mrollout_buffer_class\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstable_baselines3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRolloutBuffer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mrollout_buffer_kwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mtarget_kl\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mstats_window_size\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mtensorboard_log\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mpolicy_kwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mseed\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mdevice\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'auto'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0m_init_setup_model\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m     \n",
      "Proximal Policy Optimization algorithm (PPO) (clip version)\n",
      "\n",
      "Paper: https://arxiv.org/abs/1707.06347\n",
      "Code: This implementation borrows code from OpenAI Spinning Up (https://github.com/openai/spinningup/)\n",
      "https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail and\n",
      "Stable Baselines (PPO2 from https://github.com/hill-a/stable-baselines)\n",
      "\n",
      "Introduction to PPO: https://spinningup.openai.com/en/latest/algorithms/ppo.html\n",
      "\n",
      ":param policy: The policy model to use (MlpPolicy, CnnPolicy, ...)\n",
      ":param env: The environment to learn from (if registered in Gym, can be str)\n",
      ":param learning_rate: The learning rate, it can be a function\n",
      "    of the current progress remaining (from 1 to 0)\n",
      ":param n_steps: The number of steps to run for each environment per update\n",
      "    (i.e. rollout buffer size is n_steps * n_envs where n_envs is number of environment copies running in parallel)\n",
      "    NOTE: n_steps * n_envs must be greater than 1 (because of the advantage normalization)\n",
      "    See https://github.com/pytorch/pytorch/issues/29372\n",
      ":param batch_size: Minibatch size\n",
      ":param n_epochs: Number of epoch when optimizing the surrogate loss\n",
      ":param gamma: Discount factor\n",
      ":param gae_lambda: Factor for trade-off of bias vs variance for Generalized Advantage Estimator\n",
      ":param clip_range: Clipping parameter, it can be a function of the current progress\n",
      "    remaining (from 1 to 0).\n",
      ":param clip_range_vf: Clipping parameter for the value function,\n",
      "    it can be a function of the current progress remaining (from 1 to 0).\n",
      "    This is a parameter specific to the OpenAI implementation. If None is passed (default),\n",
      "    no clipping will be done on the value function.\n",
      "    IMPORTANT: this clipping depends on the reward scaling.\n",
      ":param normalize_advantage: Whether to normalize or not the advantage\n",
      ":param ent_coef: Entropy coefficient for the loss calculation\n",
      ":param vf_coef: Value function coefficient for the loss calculation\n",
      ":param max_grad_norm: The maximum value for the gradient clipping\n",
      ":param use_sde: Whether to use generalized State Dependent Exploration (gSDE)\n",
      "    instead of action noise exploration (default: False)\n",
      ":param sde_sample_freq: Sample a new noise matrix every n steps when using gSDE\n",
      "    Default: -1 (only sample at the beginning of the rollout)\n",
      ":param rollout_buffer_class: Rollout buffer class to use. If ``None``, it will be automatically selected.\n",
      ":param rollout_buffer_kwargs: Keyword arguments to pass to the rollout buffer on creation\n",
      ":param target_kl: Limit the KL divergence between updates,\n",
      "    because the clipping is not enough to prevent large update\n",
      "    see issue #213 (cf https://github.com/hill-a/stable-baselines/issues/213)\n",
      "    By default, there is no limit on the kl div.\n",
      ":param stats_window_size: Window size for the rollout logging, specifying the number of episodes to average\n",
      "    the reported success rate, mean episode length, and mean reward over\n",
      ":param tensorboard_log: the log location for tensorboard (if None, no logging)\n",
      ":param policy_kwargs: additional arguments to be passed to the policy on creation. See :ref:`ppo_policies`\n",
      ":param verbose: Verbosity level: 0 for no output, 1 for info messages (such as device or wrappers used), 2 for\n",
      "    debug messages\n",
      ":param seed: Seed for the pseudo random generators\n",
      ":param device: Device (cpu, cuda, ...) on which the code should be run.\n",
      "    Setting it to auto, the code will be run on the GPU if possible.\n",
      ":param _init_setup_model: Whether or not to build the network at the creation of the instance\n",
      "\u001b[1;31mFile:\u001b[0m           c:\\users\\jet wei\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py\n",
      "\u001b[1;31mType:\u001b[0m           ABCMeta\n",
      "\u001b[1;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "PPO?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba45b84",
   "metadata": {},
   "source": [
    "### Training the PPO DRL algorithm in the RL Environment to become a PPO DRL model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cd8155",
   "metadata": {},
   "source": [
    "Note that the number of timesteps/iterations/episodes to be used here to train an RL algorithm varies depending on the complexity of the RL Environment.\n",
    "\n",
    "For this tutorial's RL Environment, 'CartPole-v1', it takes about 20 000 timesteps/iterations/episodes, but for more complex RL Environments it may take up to 500 000 timesteps/iterations/episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5824d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\logs\\PPO_1\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 46   |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 44   |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 44          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 91          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008763621 |\n",
      "|    clip_fraction        | 0.094       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.687      |\n",
      "|    explained_variance   | -0.0146     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.65        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0143     |\n",
      "|    value_loss           | 47          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 44          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 137         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010382799 |\n",
      "|    clip_fraction        | 0.0847      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.661      |\n",
      "|    explained_variance   | 0.108       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12          |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0187     |\n",
      "|    value_loss           | 30.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 45          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 181         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008644449 |\n",
      "|    clip_fraction        | 0.0771      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.636      |\n",
      "|    explained_variance   | 0.228       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.2        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0159     |\n",
      "|    value_loss           | 51.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 44          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 228         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009474721 |\n",
      "|    clip_fraction        | 0.0828      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.611      |\n",
      "|    explained_variance   | 0.31        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 18          |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0198     |\n",
      "|    value_loss           | 57.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 45          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 272         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010448728 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.598      |\n",
      "|    explained_variance   | 0.518       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 26.4        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0177     |\n",
      "|    value_loss           | 55          |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 45           |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 317          |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071186107 |\n",
      "|    clip_fraction        | 0.0726       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.579       |\n",
      "|    explained_variance   | 0.528        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 19.9         |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.0122      |\n",
      "|    value_loss           | 62.3         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 45           |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 361          |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043801507 |\n",
      "|    clip_fraction        | 0.0319       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.57        |\n",
      "|    explained_variance   | 0.662        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 14.4         |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00684     |\n",
      "|    value_loss           | 44.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 45           |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 406          |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053900545 |\n",
      "|    clip_fraction        | 0.0639       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.579       |\n",
      "|    explained_variance   | 0.886        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.97         |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00938     |\n",
      "|    value_loss           | 20.6         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 45          |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 450         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008002858 |\n",
      "|    clip_fraction        | 0.0723      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.569      |\n",
      "|    explained_variance   | 0.882       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.49        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0104     |\n",
      "|    value_loss           | 26.6        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x231edf9fd10>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PPO_DRL_model.learn(total_timesteps=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549fda64",
   "metadata": {},
   "source": [
    "## 4. Save PPO DRL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f563339",
   "metadata": {},
   "outputs": [],
   "source": [
    "PPO_Model_CartPole_v1_20k = os.path.join('Training_Tutorial', 'Saved RL Models', 'PPO_Model_CartPole_v1_20k')\n",
    "PPO_DRL_model.save(PPO_Model_CartPole_v1_20k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
