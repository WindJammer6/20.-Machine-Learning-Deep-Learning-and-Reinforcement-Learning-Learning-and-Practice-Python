{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07f37ed0",
   "metadata": {},
   "source": [
    "# Reinforcement Learning (RL) with Gymnasium and Stable Baselines3 Tutorial\n",
    "Source: \n",
    "- https://www.youtube.com/watch?v=Mut_u40Sqz4&t=6144s (Nicholas Renotte) (YouTube video by Nicholas Renotte titled, \n",
    "'Reinforcement Learning in 3 Hours | Full Course using Python')\n",
    "\n",
    "Documentations:\n",
    "- Gymnasium: https://gymnasium.farama.org/ (This library provides standardized environments for developing and testing RL algorithms)\n",
    "- Stable Baselines3: https://stable-baselines3.readthedocs.io/en/master/guide/quickstart.html (This library provides a suite of pre-implemented RL algorithms based on PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db8d3f8",
   "metadata": {},
   "source": [
    "# Project 3: Custom RL Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3fc1b7",
   "metadata": {},
   "source": [
    "### What are Atari Games RL Environments?\n",
    "In Gymansium, there is a class of RL Environments called Atari Games, which refers to the classic video games from the Atari 2600 console, such as:\n",
    "- Breakout\n",
    "- Pong\n",
    "- Space Invaders\n",
    "- Q*Bert\n",
    "- Seaquest\n",
    "- Montezuma's Revenge\n",
    "\n",
    "and many more...\n",
    "\n",
    "These games are used as benchmark RL Environments for evaluating and comparing the performance of RL algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12377cc4",
   "metadata": {},
   "source": [
    "## How is an RL Environment defined?\n",
    "An RL Environment is typically modeled as the 5-tuple:\n",
    "```text\n",
    "ùëÄ=(ùëÜ,ùê¥,ùëÉ,ùëÖ,ùõæ)\n",
    "```\n",
    "\n",
    "An RL Environment is defined as 5-tuple in the framework of a Markov Decision Process (MDP):\n",
    "\n",
    "| Symbol              | Name                       | Description                                                                               |\n",
    "| ------------------- | -------------------------- | ----------------------------------------------------------------------------------------- |\n",
    "| $S$                 | **States**                 | The set of all possible states the agent can be in                                        |\n",
    "| $A$                 | **Actions**                | The set of all possible actions the agent can take                                        |\n",
    "| $P(s' \\mid s, a)$   | **Transition Probability** | The probability of moving to state $s'$ after taking action $a$ in state $s$              |\n",
    "| $R(s, a)$           | **Reward Function**        | The expected reward received after taking action $a$ in state $s$                         |\n",
    "| $\\gamma \\in [0, 1]$ | **Discount Factor**        | The factor by which future rewards are discounted (controls how far-sighted the agent is) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5c3a14",
   "metadata": {},
   "source": [
    "## How does Gymnasium represent each of these components of the RL Environment?\n",
    "**States**/**Observations** and  **Actions**  \n",
    "- Box ‚Äì n-dimensional tensor, range of values (continuous values)\n",
    "    ```\n",
    "    E.g. Box(0, 1, shape=(3,3))\n",
    "    ```\n",
    "- Discrete ‚Äì Set of items (discrete values)\n",
    "    ```\n",
    "    E.g. Discrete(3)\n",
    "    ```\n",
    "- Tuple ‚Äì Tuple of other spaces (e.g., Box or Discrete)\n",
    "    ```\n",
    "    E.g. Tuple((Discrete(2), Box(0, 100, shape=(1,))))\n",
    "    ```\n",
    "- Dict ‚Äì Dictionary of spaces (e.g., Box or Discrete)\n",
    "    ```\n",
    "    E.g. Dict({\"height\": Discrete(2), \"speed\": Box(0, 100, shape=(1,))})\n",
    "    ```\n",
    "- MultiBinary ‚Äì One-hot encoded binary values\n",
    "    ```\n",
    "    E.g. MultiBinary(4)\n",
    "    ```\n",
    "- MultiDiscrete ‚Äì Multiple discrete values\n",
    "    ```\n",
    "    E.g. MultiDiscrete([5, 2, 2])\n",
    "    ```\n",
    "\n",
    "**Transition Probability**  \n",
    "- abstracted out by the Gymmnasium library\n",
    "\n",
    "**Reward Function**  \n",
    "- abstracted out by the Gymmnasium library\n",
    "\n",
    "**Discount Factor**\n",
    "- abstracted out by the Gymmnasium library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e3a4d1",
   "metadata": {},
   "source": [
    "### What is the difference between States and Observations?\n",
    "RL agents only act on observations, not states. Optimal behavior of RL agents assumes knowledge of the underlying state (or estimates of it).\n",
    "\n",
    "| **Aspect**          | **State**                                                    | **Observation**                                                 |\n",
    "| ------------------- | ------------------------------------------------------------ | --------------------------------------------------------------- |\n",
    "| **Definition**      | The **true internal configuration** of the environment       | The **information** the agent **receives** from the environment |\n",
    "| **Completeness**    | Often assumed to be **complete** (Markov property holds)     | May be **partial**, noisy, or incomplete view of the state      |\n",
    "| **Markov Property** | A true state satisfies: future depends only on current state | Observations may not satisfy the Markov property                |\n",
    "| **Agent‚Äôs View**    | Agent may not have access to the full state                  | Agent always uses observations to decide actions                |\n",
    "| **Example**         | All object positions, velocities, and environment internals  | Camera image, radar scan, or any sensor reading                 |\n",
    "\n",
    "**MDP vs POMDP**\n",
    "- In fully observable environments (e.g., many standard RL benchmarks), the observation is equivalent to the state. This is assumed in Markov Decision Processes (MDPs).\n",
    "- In Partially Observable MDPs (POMDPs), the agent sees only observations and must infer the state using memory or belief models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46deb6f",
   "metadata": {},
   "source": [
    "## 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77af2fc7",
   "metadata": {},
   "source": [
    "**To run Gymnasium and Stable Baselines3 libraries, it is HIGHLY recommended to create a virtual environment and download the dependencies/requirements in the virtual environment seperately to prevent conflicts in libraries!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d327991e",
   "metadata": {},
   "source": [
    "### How to set up a virtual environment in VS Code?\n",
    "1. **Create a virtual environment**\n",
    "    ```bash\n",
    "    python -m venv venv\n",
    "    ```\n",
    "    This creates a folder named venv/ containing the isolated environment.\n",
    "\n",
    "2. **Activate the virtual environment**\n",
    "\n",
    "    For Windows:\n",
    "    ```bash\n",
    "    .\\venv\\Scripts\\activate\n",
    "    ```\n",
    "    For macOS/Linux:\n",
    "    ```bash\n",
    "    source venv/bin/activate\n",
    "    ```\n",
    "    You‚Äôll know it‚Äôs activated when your terminal prompt changes to show (venv).\n",
    "\n",
    "3. **Now you can install dependencies inside the virtual environment!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26538f5a",
   "metadata": {},
   "source": [
    "### What dependencies/requirements to download? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649db53c",
   "metadata": {},
   "source": [
    "**For Gymnasium library**\n",
    "```bash\n",
    "pip install gymnasium\n",
    "```\n",
    "\n",
    "**For Stable Baselines3 library**\n",
    "```bash\n",
    "pip install stable-baselines3[extra]\n",
    "```\n",
    "\n",
    "**For ALE (Arcade Learning Environment) package**  \n",
    "The current newer version of Gymnasium library no longer include Atari Games RL Environments anymore by default. To use these Atari Games RL Environments with Gymnasium, you need to download a seperate dependency/package, the ALE (Arcade Learning Environment) package.\n",
    "```bash\n",
    "pip install autorom[accept-rom-license]\n",
    "pip install ale-py\n",
    "```\n",
    "\n",
    "Source(s):\n",
    "- https://github.com/AndreM96/Stable_Baseline3_Gymnasium_Tutorial (AndreM96 on Github)\n",
    "- https://www.youtube.com/watch?v=Mut_u40Sqz4&t=6144s (one of the comments under the YouTube video by Nicholas Renotte titled, 'Reinforcement Learning in 3 Hours | Full Course using Python')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7bbf80",
   "metadata": {},
   "source": [
    "Just for demonstration purposes, the RL algorithm that we will be using here is the Advantage Actor-Critic (A2C) DRL algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b9b41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Gymnasium-related dependencies\n",
    "import gymnasium as gym\n",
    "from gymnasium import Env\n",
    "from gymnasium.spaces import Discrete, Box, Dict, Tuple, MultiBinary, MultiDiscrete\n",
    "\n",
    "# Import Stable Baselines3-related dependencies\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "# Import helper dependencies\n",
    "import numpy as np\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8953d5ca",
   "metadata": {},
   "source": [
    "## 2. Types of Gymnasium's Spaces\n",
    "It represents the structure and boundaries of either:\n",
    "- Observation space: What the agent sees (e.g. images, coordinates, sensors)\n",
    "- State space: All possible configurations the environment can be in (often implied)\n",
    "- Action space: What actions the agent can take (e.g. move left/right, accelerate)\n",
    "\n",
    "As seen from the section above, 'How does Gymnasium represent each of these components of the RL Environment?', there are 6 types of Gymnasium Spaces:\n",
    "1. Box ‚Äì n-dimensional tensor, range of values (continuous values)\n",
    "    ```\n",
    "    E.g. Box(0, 1, shape=(3,3))\n",
    "    ```\n",
    "2. Discrete ‚Äì Set of items (discrete values)\n",
    "    ```\n",
    "    E.g. Discrete(3)\n",
    "    ```\n",
    "3. Tuple ‚Äì Tuple of other spaces (e.g., Box or Discrete)\n",
    "    ```\n",
    "    E.g. Tuple((Discrete(2), Box(0, 100, shape=(1,))))\n",
    "    ```\n",
    "4. Dict ‚Äì Dictionary of spaces (e.g., Box or Discrete)\n",
    "    ```\n",
    "    E.g. Dict({\"height\": Discrete(2), \"speed\": Box(0, 100, shape=(1,))})\n",
    "    ```\n",
    "5. MultiBinary ‚Äì One-hot encoded binary values\n",
    "    ```\n",
    "    E.g. MultiBinary(4)\n",
    "    ```\n",
    "6. MultiDiscrete ‚Äì Multiple discrete values\n",
    "    ```\n",
    "    E.g. MultiDiscrete([5, 2, 2])\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2a6c11",
   "metadata": {},
   "source": [
    "### 1. Box Space\n",
    "```\n",
    "Box(low, high, shape, dtype=float)\n",
    "E.g. Box(0, 1, shape=(3,3))\n",
    "```\n",
    "| **Parameter**      | **Value**         | **Meaning**                                                                 |\n",
    "| ------------------ | ----------------- | --------------------------------------------------------------------------- |\n",
    "| `low`              | `0`               | Minimum possible value for each element in the box (can be scalar or array) |\n",
    "| `high`             | `1`               | Maximum possible value for each element (same shape as `low` or scalar)     |\n",
    "| `shape`            | `(3, 3)`          | Shape of the space ‚Äî this creates a **3√ó3 matrix** space                    |\n",
    "| `dtype` (optional) | `float` (default) | Data type for the elements (e.g., `np.float32`, `np.int32`)                 |\n",
    "\n",
    "Best for: Continuous control tasks, like robotics, self-driving cars, or any numeric sensor input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "82dfc269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(0.0, 1.0, (3, 3), float32)\n"
     ]
    }
   ],
   "source": [
    "print(Box(0, 1, shape=(3,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "1fd07b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.38299504 0.3242162  0.0487468 ]\n",
      " [0.83433515 0.63927984 0.12781315]\n",
      " [0.95965    0.796533   0.12002907]]\n"
     ]
    }
   ],
   "source": [
    "print(Box(0, 1, shape=(3,3)).sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905272e2",
   "metadata": {},
   "source": [
    "### 2. Discrete Space\n",
    "```\n",
    "Discrete(n)\n",
    "E.g. Discrete(3)\n",
    "```\n",
    "| **Parameter** | **Value** | **Meaning**                                          |\n",
    "| ------------- | --------- | ---------------------------------------------------- |\n",
    "| `n`           | `3`       | The number of **distinct values**, from `0` to `n-1` |\n",
    "\n",
    "Best for: Single categorical choices (e.g., move directions, menu options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "2f622323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(3)\n"
     ]
    }
   ],
   "source": [
    "print(Discrete(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e0526fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Discrete.sample of Discrete(3)>\n"
     ]
    }
   ],
   "source": [
    "print(Discrete(3).sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909cb4b2",
   "metadata": {},
   "source": [
    "### 3. Tuple Space\n",
    "```\n",
    "E.g. Tuple((Discrete(2), Box(0, 100, shape=(1,))))\n",
    "```\n",
    "| **Parameter** | **Value**                              | **Meaning**                                                     |\n",
    "| ------------- | -------------------------------------- | --------------------------------------------------------------- |\n",
    "| `spaces`      | `(Discrete(2), Box(0, 100, shape=(1,)))` | A tuple of **independent spaces** (can mix Discrete, Box, etc.) |\n",
    "\n",
    "Best for: Environments with multi-part observations or actions where each part is different in type or range\n",
    "\n",
    "Example: (button_pressed, sensor_readings)\n",
    "\n",
    "IMPORTANT NOTE:  \n",
    "Tuple Space is not supported by Stable Baselines3! (Its the only one, all other spaces types are supported by Stable Baselines3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "07f0c2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuple(Discrete(3), Box(0.0, 1.0, (3, 3), float32))\n"
     ]
    }
   ],
   "source": [
    "print(Tuple((Discrete(3), Box(0,1, shape=(3,3)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "2a9ce265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(np.int64(2), array([[0.57508886, 0.3206607 , 0.20824908],\n",
      "       [0.13850348, 0.6429767 , 0.4782226 ],\n",
      "       [0.08177763, 0.7019443 , 0.59392303]], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "print(Tuple((Discrete(3), Box(0,1, shape=(3,3)))).sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff13484",
   "metadata": {},
   "source": [
    "### 4. Dict Space\n",
    "```\n",
    "E.g. Dict({\"height\": Discrete(2), \"speed\": Box(0, 100, shape=(1,))})\n",
    "```\n",
    "| **Parameter** | **Value**                                     | **Meaning**                                                 |\n",
    "| ------------- | --------------------------------------------- | ----------------------------------------------------------- |\n",
    "| `spaces`      | `{\"image\": Box(...), \"speed\": Discrete(...)}` | A **dictionary of named spaces**, for structured data input |\n",
    "\n",
    "Best for: Named observation components (e.g., image + metadata, or lidar + speed + GPS)\n",
    "\n",
    "Example: structured state inputs like {\"camera\": image, \"position\": vector}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c966d9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict('height': Discrete(2), 'speed': Box(0.0, 100.0, (1,), float32))\n"
     ]
    }
   ],
   "source": [
    "print(Dict({\"height\" : Discrete(2), \"speed\" : Box(0, 100, shape={1,})}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "59c2f5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'height': np.int64(0), 'speed': array([51.066807], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "print(Dict({\"height\" : Discrete(2), \"speed\" : Box(0, 100, shape={1,})}).sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26ea3cf",
   "metadata": {},
   "source": [
    "### 5. MultiBinary Space\n",
    "```\n",
    "MultiBinary(n)\n",
    "E.g. MultiBinary(4)\n",
    "```\n",
    "| **Parameter** | **Value** | **Meaning**                                 |\n",
    "| ------------- | --------- | ------------------------------------------- |\n",
    "| `n`           | `4`       | Number of binary variables (each is 0 or 1) |\n",
    "\n",
    "Best for: Multiple binary options (e.g., toggles, flags, presence/absence of features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "eb2a75df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiBinary(4)\n"
     ]
    }
   ],
   "source": [
    "print(MultiBinary(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "a704ca6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(MultiBinary(4).sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e91cbe",
   "metadata": {},
   "source": [
    "### 6. MultiDiscrete Space\n",
    "```\n",
    "MultiDiscrete([n‚ÇÅ, n‚ÇÇ, ..., n‚Çñ]), where 'nvec = [n‚ÇÅ, n‚ÇÇ, ..., n‚Çñ]'\n",
    "E.g. MultiDiscrete([5, 2, 2])\n",
    "```\n",
    "| **Parameter** | **Value**   | **Meaning**                                                                    |\n",
    "| ------------- | ----------- | ------------------------------------------------------------------------------ |\n",
    "| `nvec`        | `[5, 2, 2]` | List of number of categories per variable. Each variable ranges from 0 to n·µ¢‚àí1 |\n",
    "\n",
    "Best for: Multi-dimensional categorical actions or observations, where each slot is an independent discrete choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "2fc12fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiDiscrete([5 2 2])\n"
     ]
    }
   ],
   "source": [
    "print(MultiDiscrete([5,2,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "8f45c8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(MultiDiscrete([5,2,2]).sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a841db34",
   "metadata": {},
   "source": [
    "## 3. Building a Custom RL Environment and testing if it works with a baseline algorithm that takes random actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f13962",
   "metadata": {},
   "source": [
    "Just for demonstration purposes, the custom RL Environment that we will try to build is a Shower RL Environment, which at a high level idea:\n",
    "- Build an RL agent that gives us the best shower possible\n",
    "- The RL agent randomly sets the shower temperature\n",
    "- The ideal shower temperature is between 37 to 39 degrees (we know this detail, but our RL agent dosen't)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d21f8dd",
   "metadata": {},
   "source": [
    "### What is the 5-tuple MDP components of our MDP RL Environment?\n",
    "\n",
    "**States**/**Observations**\n",
    "- a continuous (set shower temperature) value between 0 to 100 - represented by a Box Space\n",
    "\n",
    "Type: Box(0, 100)\n",
    "| Num | Observation         | Min | Max | Description                           |\n",
    "| --- | ------------------- | --- | --- | ------------------------------------- |\n",
    "| 0   | Shower temperature  | 0   | 100 | Range of possible shower temperatures |\n",
    "\n",
    "**Actions**  \n",
    "- 3 discrete actions - represented by a Discrete Space\n",
    "1. Increase shower temperature by 1 - represented by a value of 2\n",
    "2. Maintain shower temperature - represented by a value of 1\n",
    "3. Decrease shower temperature by 1 - represented by a value of 0\n",
    "\n",
    "Type: Discrete(3)\n",
    "| **Index** | **Action Name**               | **Meaning**                      |\n",
    "| --------- | ----------------------------- | -------------------------------- |\n",
    "| 0         | Decrease Shower Temperature   | Decrease Shower Temperature by 1 |\n",
    "| 1         | Maintain shower temperature   | No change to shower temperature  |\n",
    "| 2         | Increase shower temperature   | Increase shower temperature by 1 |\n",
    "\n",
    "**Transition Probability**  \n",
    "- abstracted out by the Gymmnasium library\n",
    "\n",
    "**Reward Function**  \n",
    "- if shower temperature is not between 37 to 39 degrees inclusive, reward -1\n",
    "- else, reward +1\n",
    "\n",
    "**Discount Factor**\n",
    "- abstracted out by the Gymmnasium library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9518ab",
   "metadata": {},
   "source": [
    "### Building the Custom RL Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "bccdc18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShowerEnv(Env):\n",
    "    def __init__(self):\n",
    "        self.action_space = Discrete(3)\n",
    "        self.observation_space = Box(low=np.array([0]), high=np.array([100]))\n",
    "        self.state = 38 + random.randint(-3, 3)         # Initialising the initial state of the RL Environment\n",
    "        self.shower_length = 60\n",
    "\n",
    "    def step(self, action):\n",
    "        # Apply action/set shower temperature\n",
    "        self.state += action - 1\n",
    "\n",
    "        # Decrease 'shower_length' time\n",
    "        self.shower_length -= 1\n",
    "\n",
    "        # Calculate Reward with Reward Function\n",
    "        reward = 0\n",
    "        if self.state >= 37 and self.state <= 39:\n",
    "            reward += 1\n",
    "        else:\n",
    "            reward -= 1\n",
    "\n",
    "        if self.shower_length <= 0:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "        truncated = False\n",
    "        info = {}\n",
    "\n",
    "        return self.state, reward, done, truncated, info\n",
    "\n",
    "    # Optional feature to visualise the RL agent in the RL Environment. Can be done using pygame. Won't be covered in this\n",
    "    # tutorial\n",
    "    def render(self):\n",
    "        pass\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        # Re-initialising the initial state of the RL Environment\n",
    "        self.state = np.array([38 + random.randint(-3, 3)]).astype(float)\n",
    "        self.shower_length = 60\n",
    "\n",
    "        info = {}\n",
    "\n",
    "        return self.state, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "586f815b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(0.0, 100.0, (1,), float32)\n",
      "Discrete(3)\n",
      "(array([39.]), {})\n"
     ]
    }
   ],
   "source": [
    "# Understanding the state and action spaces used in the Custom Shower RL Environment\n",
    "env = ShowerEnv()\n",
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "print(env.reset())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15655444",
   "metadata": {},
   "source": [
    "### Testing the Custom RL Environment if it works with a baseline algorithm that takes random actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "f97c459a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State: [35.]\n",
      "Episode: 0 Score: -10\n",
      "Initial State: [39.]\n",
      "Episode: 1 Score: -6\n",
      "Initial State: [40.]\n",
      "Episode: 2 Score: -50\n",
      "Initial State: [41.]\n",
      "Episode: 3 Score: -60\n",
      "Initial State: [37.]\n",
      "Episode: 4 Score: -48\n",
      "Initial State: [41.]\n",
      "Episode: 5 Score: 0\n"
     ]
    }
   ],
   "source": [
    "env = ShowerEnv()\n",
    "\n",
    "episodes = 5\n",
    "for episode in range(0, episodes+1):\n",
    "    # Initialise starting state of the RL agent in the RL Environment before an episode, done to false, and starting \n",
    "    # episode score to 0\n",
    "    obs, _ = env.reset()\n",
    "    print(f\"Initial State: {obs}\")\n",
    "    done = False\n",
    "    episode_score = 0\n",
    "\n",
    "    # During an episode:\n",
    "    while not done:\n",
    "        env.render()\n",
    "        # RL agent determines action to take\n",
    "        # - In this case, we are randomly sampling an action to take by our RL agent in the RL Environment (this line of\n",
    "        #   code defines that baseline algorithm that takes random actions (instead of an RL algorithm))\n",
    "        action = env.action_space.sample()\n",
    "        # RL Environment generates the next state and reward gained upon taking the action in the current state\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        # Append the reward gained upon taking the action in the current state to the cumulative episode date\n",
    "        episode_score += reward\n",
    "\n",
    "    print(f\"Episode: {episode} Score: {episode_score}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f31e19a",
   "metadata": {},
   "source": [
    "## 3. Vectorise RL Environment and Train an A2C DRL algorithm in a RL Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3416be12",
   "metadata": {},
   "source": [
    "### What is an Reinforcement Learning (RL) algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e22779",
   "metadata": {},
   "source": [
    "An RL algorithm involves an agent performing actions in an RL environment, receiving rewards or penalties based on those actions, and adjusting its behavior accordingly. This loop helps the agent improve its decision-making over time to maximize the cumulative reward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc16bfc8",
   "metadata": {},
   "source": [
    "### How does a Reinforcement Learning (RL) algorithm 'learn'?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f55666",
   "metadata": {},
   "source": [
    "In ML and DL, we learnt that ML/DL algorithms 'learn' by updating the ML/DL algorithm's weights and biases as more datas are fed into the ML/DL algorithm, and after many iterations of training, it makes accurate predictions. \n",
    "\n",
    "**This is no different in RL.**\n",
    "\n",
    "In RL, the RL algorithms uses various architectures to 'learn' by updating the RL algorithm's weights and biases as it interacts more with the RL Environment (via the reward mechanism). The 'learning' architecture used also defines whether a RL algorithm is a **Classical RL algorithm** or a **Deep RL (DRL) algorithm**.\n",
    "\n",
    "**Classical RL algorithm learning architectures**  \n",
    "Uses tables or simple functions:\n",
    "| Type                          | Description                                                                      | Example             |\n",
    "| ----------------------------- | -------------------------------------------------------------------------------- | ------------------- |\n",
    "| **Tabular policy**            | Table stores the best action for each discrete state                             | `œÄ[s] = a`          |\n",
    "| **Tabular stochastic policy** | Table of probabilities for each action in each state                             | `œÄ[a][s] = P(a \\| s)` |\n",
    "| **Value-based methods**       | Use a value table (e.g., Q-table) and derive policy as `œÄ(s) = argmax Q(s,a)`    | Q-Learning          |\n",
    "| **Policy iteration**          | Alternates between evaluating a policy and improving it based on value estimates | Dynamic Programming |      |\n",
    "| **Function approximation**    | Uses linear models or tile coding to generalize across large state spaces        | `œÄ(s) = Œ∏·µÄœÜ(s)`     |\n",
    "\n",
    "**Deep RL (DRL) algorithm learning architectures**  \n",
    "Uses neural networks or its variants,\n",
    "- FNN/MLP\n",
    "- CNN\n",
    "- RNN\n",
    "- LSTM\n",
    "- GRU\n",
    "\n",
    "In RL, after many iterations of training, it makes accurate predictions, more specifically, it behaves better/takes better actions. \n",
    "\n",
    "These RL algorithm 'learning' architectures is also called **Policy**, which defines how the agent chooses actions based on its current state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6266f8a",
   "metadata": {},
   "source": [
    "### What does a Vectorised RL Environment mean?\n",
    "Vectorized RL Environments are RL Environments that can be made to run in parallel, allowing multiple simulations at once to increase training speed of the RL algorithm.\n",
    "\n",
    "A non-vectorized RL Environment does not allow for being made to run in parallel (only one simulation can run at a time).\n",
    "\n",
    "In Gymnasium, some RL Environments are vectorized by default (e.g. Breakout), while others are not (e.g. CartPole). But when training a RL algorithm from Stable Baselines3, it is required for the RL Environment to be vectorized as well (even if you dont intend to run them in parallel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "0a404bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since when we create the Custom RL Environment with the 'Env' super class from Gymnasium, it automatically\n",
    "# wraps the Custom RL Environment in a dummy vectorised RL Environment already, hence there is no need\n",
    "# to vectorise it again "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0605d69b",
   "metadata": {},
   "source": [
    "### For logging purposes of the training process of the PPO DRL algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "eca901c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training_Project_3_Custom\\logs\n"
     ]
    }
   ],
   "source": [
    "# Stating the path where we want to store our training logs files in the local folder './Training_Project_3_Custom/logs'\n",
    "log_path = os.path.join('Training_Project_3_Custom', 'logs')\n",
    "print(log_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab83cefd",
   "metadata": {},
   "source": [
    "### Creating the PPO DRL algorithm in the RL Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "dbbd9bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "# What does each of the parameters in the 'PPO' DRL algorithm class mean?\n",
    "# - 'policy' (e.g. 'MlpPolicy'  - refers to the learning architecture used a the policy of the RL algorithm, which in this\n",
    "#               or 'CnnPolicy')   is FNN/MLP\n",
    "# - 'env'                       - refers to the RL environment to train the RL algorithm in\n",
    "# - 'verbose'                   - controls how much information is printed to the console/log during training\n",
    "#                                 -> 'verbose=0' means 'Silent', no output at all\n",
    "#                                 -> 'verbose=1' means 'Info', shows key training events: episode rewards, updates, losses, etc.\n",
    "#                                 -> 'verbose=2' means 'Debug' shows more detailed info like hyperparameters, rollout steps, and internal logs\n",
    "# - 'tensorboard_log'           - states to do the training logging in Tensorboard\n",
    "PPO_DRL_model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "e1e82ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInit signature:\u001b[39m\n",
      "PPO(\n",
      "    policy: Union[str, type[stable_baselines3.common.policies.ActorCriticPolicy]],\n",
      "    env: Union[gymnasium.core.Env, ForwardRef(\u001b[33m'VecEnv'\u001b[39m), str],\n",
      "    learning_rate: Union[float, Callable[[float], float]] = \u001b[32m0.0003\u001b[39m,\n",
      "    n_steps: int = \u001b[32m2048\u001b[39m,\n",
      "    batch_size: int = \u001b[32m64\u001b[39m,\n",
      "    n_epochs: int = \u001b[32m10\u001b[39m,\n",
      "    gamma: float = \u001b[32m0.99\u001b[39m,\n",
      "    gae_lambda: float = \u001b[32m0.95\u001b[39m,\n",
      "    clip_range: Union[float, Callable[[float], float]] = \u001b[32m0.2\u001b[39m,\n",
      "    clip_range_vf: Union[NoneType, float, Callable[[float], float]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    normalize_advantage: bool = \u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "    ent_coef: float = \u001b[32m0.0\u001b[39m,\n",
      "    vf_coef: float = \u001b[32m0.5\u001b[39m,\n",
      "    max_grad_norm: float = \u001b[32m0.5\u001b[39m,\n",
      "    use_sde: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "    sde_sample_freq: int = -\u001b[32m1\u001b[39m,\n",
      "    rollout_buffer_class: Optional[type[stable_baselines3.common.buffers.RolloutBuffer]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    rollout_buffer_kwargs: Optional[dict[str, Any]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    target_kl: Optional[float] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    stats_window_size: int = \u001b[32m100\u001b[39m,\n",
      "    tensorboard_log: Optional[str] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    policy_kwargs: Optional[dict[str, Any]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    verbose: int = \u001b[32m0\u001b[39m,\n",
      "    seed: Optional[int] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    device: Union[torch.device, str] = \u001b[33m'auto'\u001b[39m,\n",
      "    _init_setup_model: bool = \u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      ")\n",
      "\u001b[31mDocstring:\u001b[39m     \n",
      "Proximal Policy Optimization algorithm (PPO) (clip version)\n",
      "\n",
      "Paper: https://arxiv.org/abs/1707.06347\n",
      "Code: This implementation borrows code from OpenAI Spinning Up (https://github.com/openai/spinningup/)\n",
      "https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail and\n",
      "Stable Baselines (PPO2 from https://github.com/hill-a/stable-baselines)\n",
      "\n",
      "Introduction to PPO: https://spinningup.openai.com/en/latest/algorithms/ppo.html\n",
      "\n",
      ":param policy: The policy model to use (MlpPolicy, CnnPolicy, ...)\n",
      ":param env: The environment to learn from (if registered in Gym, can be str)\n",
      ":param learning_rate: The learning rate, it can be a function\n",
      "    of the current progress remaining (from 1 to 0)\n",
      ":param n_steps: The number of steps to run for each environment per update\n",
      "    (i.e. rollout buffer size is n_steps * n_envs where n_envs is number of environment copies running in parallel)\n",
      "    NOTE: n_steps * n_envs must be greater than 1 (because of the advantage normalization)\n",
      "    See https://github.com/pytorch/pytorch/issues/29372\n",
      ":param batch_size: Minibatch size\n",
      ":param n_epochs: Number of epoch when optimizing the surrogate loss\n",
      ":param gamma: Discount factor\n",
      ":param gae_lambda: Factor for trade-off of bias vs variance for Generalized Advantage Estimator\n",
      ":param clip_range: Clipping parameter, it can be a function of the current progress\n",
      "    remaining (from 1 to 0).\n",
      ":param clip_range_vf: Clipping parameter for the value function,\n",
      "    it can be a function of the current progress remaining (from 1 to 0).\n",
      "    This is a parameter specific to the OpenAI implementation. If None is passed (default),\n",
      "    no clipping will be done on the value function.\n",
      "    IMPORTANT: this clipping depends on the reward scaling.\n",
      ":param normalize_advantage: Whether to normalize or not the advantage\n",
      ":param ent_coef: Entropy coefficient for the loss calculation\n",
      ":param vf_coef: Value function coefficient for the loss calculation\n",
      ":param max_grad_norm: The maximum value for the gradient clipping\n",
      ":param use_sde: Whether to use generalized State Dependent Exploration (gSDE)\n",
      "    instead of action noise exploration (default: False)\n",
      ":param sde_sample_freq: Sample a new noise matrix every n steps when using gSDE\n",
      "    Default: -1 (only sample at the beginning of the rollout)\n",
      ":param rollout_buffer_class: Rollout buffer class to use. If ``None``, it will be automatically selected.\n",
      ":param rollout_buffer_kwargs: Keyword arguments to pass to the rollout buffer on creation\n",
      ":param target_kl: Limit the KL divergence between updates,\n",
      "    because the clipping is not enough to prevent large update\n",
      "    see issue #213 (cf https://github.com/hill-a/stable-baselines/issues/213)\n",
      "    By default, there is no limit on the kl div.\n",
      ":param stats_window_size: Window size for the rollout logging, specifying the number of episodes to average\n",
      "    the reported success rate, mean episode length, and mean reward over\n",
      ":param tensorboard_log: the log location for tensorboard (if None, no logging)\n",
      ":param policy_kwargs: additional arguments to be passed to the policy on creation. See :ref:`ppo_policies`\n",
      ":param verbose: Verbosity level: 0 for no output, 1 for info messages (such as device or wrappers used), 2 for\n",
      "    debug messages\n",
      ":param seed: Seed for the pseudo random generators\n",
      ":param device: Device (cpu, cuda, ...) on which the code should be run.\n",
      "    Setting it to auto, the code will be run on the GPU if possible.\n",
      ":param _init_setup_model: Whether or not to build the network at the creation of the instance\n",
      "\u001b[31mFile:\u001b[39m           c:\\users\\jet wei\\documents\\jetwei\\personal programming stuff\\reinforcement learning\\venv\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py\n",
      "\u001b[31mType:\u001b[39m           ABCMeta\n",
      "\u001b[31mSubclasses:\u001b[39m     "
     ]
    }
   ],
   "source": [
    "PPO?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba45b84",
   "metadata": {},
   "source": [
    "### Training the PPO DRL algorithm in the RL Environment to become a PPO DRL model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cd8155",
   "metadata": {},
   "source": [
    "Note that the number of timesteps/iterations/episodes to be used here to train an RL algorithm varies depending on the complexity of the RL Environment.\n",
    "\n",
    "For this tutorial's RL Environment, 'Breakout-v0', it is moderately complex and should take about 100 000 to 200 000 timesteps/iterations/episodes compared to the simpler 'CartPole-v1' RL Environment which should only take about 20 000 timesteps/iterations/episodes, but for more complex RL Environments it may take up to 500 000 timesteps/iterations/episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "7c5824d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training_Project_3_Custom\\logs\\PPO_2\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 60       |\n",
      "|    ep_rew_mean     | 53.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 2467     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 53.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1494        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023102708 |\n",
      "|    clip_fraction        | 0.166       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.815      |\n",
      "|    explained_variance   | 2.34e-05    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 39.6        |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | 0.00996     |\n",
      "|    value_loss           | 87.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 52.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1338        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016561395 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.785      |\n",
      "|    explained_variance   | 5.3e-06     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 50.8        |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | 0.00905     |\n",
      "|    value_loss           | 96          |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 60         |\n",
      "|    ep_rew_mean          | 48         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1272       |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01170183 |\n",
      "|    clip_fraction        | 0.15       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.789     |\n",
      "|    explained_variance   | -6.08e-06  |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 46.4       |\n",
      "|    n_updates            | 520        |\n",
      "|    policy_gradient_loss | 0.00746    |\n",
      "|    value_loss           | 94.9       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 42.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1232        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007943457 |\n",
      "|    clip_fraction        | 0.165       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.786      |\n",
      "|    explained_variance   | -4.37e-05   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 47.9        |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.000653   |\n",
      "|    value_loss           | 105         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 60           |\n",
      "|    ep_rew_mean          | 41           |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1173         |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0115533555 |\n",
      "|    clip_fraction        | 0.171        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.801       |\n",
      "|    explained_variance   | -9.67e-05    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 32.5         |\n",
      "|    n_updates            | 540          |\n",
      "|    policy_gradient_loss | 0.00596      |\n",
      "|    value_loss           | 80.7         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 45.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1156        |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030364022 |\n",
      "|    clip_fraction        | 0.198       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.806      |\n",
      "|    explained_variance   | -2.97e-05   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 35.2        |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | 0.00971     |\n",
      "|    value_loss           | 72.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 49.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1149        |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016514441 |\n",
      "|    clip_fraction        | 0.156       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.818      |\n",
      "|    explained_variance   | 3.58e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 45.3        |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | 0.00806     |\n",
      "|    value_loss           | 84.8        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 60           |\n",
      "|    ep_rew_mean          | 52           |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1086         |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066781514 |\n",
      "|    clip_fraction        | 0.173        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.806       |\n",
      "|    explained_variance   | -1.66e-05    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 45.6         |\n",
      "|    n_updates            | 570          |\n",
      "|    policy_gradient_loss | 0.00952      |\n",
      "|    value_loss           | 91.7         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 50.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1054        |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 19          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020315668 |\n",
      "|    clip_fraction        | 0.195       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.817      |\n",
      "|    explained_variance   | 1.49e-06    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 36.9        |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | 0.0127      |\n",
      "|    value_loss           | 93.3        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 60           |\n",
      "|    ep_rew_mean          | 50.8         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1043         |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 21           |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071591353 |\n",
      "|    clip_fraction        | 0.208        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.799       |\n",
      "|    explained_variance   | -1.43e-05    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 53.3         |\n",
      "|    n_updates            | 590          |\n",
      "|    policy_gradient_loss | 0.0162       |\n",
      "|    value_loss           | 99.4         |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 60         |\n",
      "|    ep_rew_mean          | 39.5       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1022       |\n",
      "|    iterations           | 12         |\n",
      "|    time_elapsed         | 24         |\n",
      "|    total_timesteps      | 24576      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01960819 |\n",
      "|    clip_fraction        | 0.181      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.812     |\n",
      "|    explained_variance   | -7.03e-06  |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 60         |\n",
      "|    n_updates            | 600        |\n",
      "|    policy_gradient_loss | 0.00641    |\n",
      "|    value_loss           | 97.3       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 60         |\n",
      "|    ep_rew_mean          | 33         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 999        |\n",
      "|    iterations           | 13         |\n",
      "|    time_elapsed         | 26         |\n",
      "|    total_timesteps      | 26624      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02673335 |\n",
      "|    clip_fraction        | 0.238      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.805     |\n",
      "|    explained_variance   | -1.55e-05  |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 38.4       |\n",
      "|    n_updates            | 610        |\n",
      "|    policy_gradient_loss | -0.00517   |\n",
      "|    value_loss           | 87.7       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 32.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 980         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011342466 |\n",
      "|    clip_fraction        | 0.206       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.797      |\n",
      "|    explained_variance   | 0.00131     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 34.6        |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.000612   |\n",
      "|    value_loss           | 61.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 38          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 964         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 31          |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012100644 |\n",
      "|    clip_fraction        | 0.206       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.802      |\n",
      "|    explained_variance   | 0.00194     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 40.4        |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | 0.0114      |\n",
      "|    value_loss           | 67.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 41.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 967         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 33          |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011715429 |\n",
      "|    clip_fraction        | 0.184       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.804      |\n",
      "|    explained_variance   | 0.00161     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 38.8        |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.00667    |\n",
      "|    value_loss           | 71.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 42.3        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 953         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 36          |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.060336746 |\n",
      "|    clip_fraction        | 0.234       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.79       |\n",
      "|    explained_variance   | 0.00166     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 39.3        |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | 0.0132      |\n",
      "|    value_loss           | 60          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 48.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 947         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 38          |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023871467 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.799      |\n",
      "|    explained_variance   | 0.0021      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 36.2        |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | 0.00499     |\n",
      "|    value_loss           | 73          |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 60         |\n",
      "|    ep_rew_mean          | 53.8       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 945        |\n",
      "|    iterations           | 19         |\n",
      "|    time_elapsed         | 41         |\n",
      "|    total_timesteps      | 38912      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02154931 |\n",
      "|    clip_fraction        | 0.217      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.816     |\n",
      "|    explained_variance   | 0.000249   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 35.5       |\n",
      "|    n_updates            | 670        |\n",
      "|    policy_gradient_loss | 0.015      |\n",
      "|    value_loss           | 78.5       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 56.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 932         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 43          |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012623623 |\n",
      "|    clip_fraction        | 0.204       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.793      |\n",
      "|    explained_variance   | 1.52e-05    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 68.1        |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | 0.0144      |\n",
      "|    value_loss           | 92.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 53.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 927         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 46          |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010558475 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.749      |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 44.2        |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | 0.0123      |\n",
      "|    value_loss           | 99.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 53.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 923         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 48          |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007477859 |\n",
      "|    clip_fraction        | 0.203       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.764      |\n",
      "|    explained_variance   | -8.34e-06   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 58.4        |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | 0.014       |\n",
      "|    value_loss           | 104         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 52.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 918         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 51          |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010321852 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.762      |\n",
      "|    explained_variance   | -2.38e-06   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 49.2        |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | 0.0122      |\n",
      "|    value_loss           | 101         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 54.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 915         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 53          |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016081555 |\n",
      "|    clip_fraction        | 0.19        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.76       |\n",
      "|    explained_variance   | -2.5e-06    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 64.8        |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | 0.0135      |\n",
      "|    value_loss           | 107         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 50.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 906         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 56          |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015407434 |\n",
      "|    clip_fraction        | 0.205       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.754      |\n",
      "|    explained_variance   | -7.39e-06   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 46.1        |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | 0.00925     |\n",
      "|    value_loss           | 103         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 49          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 893         |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 59          |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011480659 |\n",
      "|    clip_fraction        | 0.178       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.749      |\n",
      "|    explained_variance   | -7.27e-06   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 34.8        |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | 0.0104      |\n",
      "|    value_loss           | 99.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 47.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 890         |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 62          |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020321045 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.741      |\n",
      "|    explained_variance   | 3.17e-05    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 46.5        |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | 0.00683     |\n",
      "|    value_loss           | 94.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 51.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 888         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 64          |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013931857 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.694      |\n",
      "|    explained_variance   | 8.61e-05    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 41.6        |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | 0.0101      |\n",
      "|    value_loss           | 92.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 52          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 889         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 66          |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014226779 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.695      |\n",
      "|    explained_variance   | 2.21e-05    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 45.1        |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | 0.00923     |\n",
      "|    value_loss           | 95.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 52.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 69          |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012682546 |\n",
      "|    clip_fraction        | 0.166       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.71       |\n",
      "|    explained_variance   | 5.84e-06    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 42          |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | 0.0104      |\n",
      "|    value_loss           | 96.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 60           |\n",
      "|    ep_rew_mean          | 48.6         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 31           |\n",
      "|    time_elapsed         | 71           |\n",
      "|    total_timesteps      | 63488        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069483244 |\n",
      "|    clip_fraction        | 0.155        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.704       |\n",
      "|    explained_variance   | 2.12e-05     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 45.5         |\n",
      "|    n_updates            | 790          |\n",
      "|    policy_gradient_loss | 0.00839      |\n",
      "|    value_loss           | 102          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 45.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 879         |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 74          |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014126303 |\n",
      "|    clip_fraction        | 0.151       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.718      |\n",
      "|    explained_variance   | 2.32e-05    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 54.4        |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | 0.0116      |\n",
      "|    value_loss           | 93.6        |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 60        |\n",
      "|    ep_rew_mean          | 43.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 880       |\n",
      "|    iterations           | 33        |\n",
      "|    time_elapsed         | 76        |\n",
      "|    total_timesteps      | 67584     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0327398 |\n",
      "|    clip_fraction        | 0.179     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.712    |\n",
      "|    explained_variance   | 0.000448  |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 37        |\n",
      "|    n_updates            | 810       |\n",
      "|    policy_gradient_loss | 0.00712   |\n",
      "|    value_loss           | 89.3      |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 47.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 875         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 79          |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013427323 |\n",
      "|    clip_fraction        | 0.153       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.7        |\n",
      "|    explained_variance   | 0.000692    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 46.9        |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | 0.00504     |\n",
      "|    value_loss           | 82          |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 60           |\n",
      "|    ep_rew_mean          | 51.8         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 873          |\n",
      "|    iterations           | 35           |\n",
      "|    time_elapsed         | 82           |\n",
      "|    total_timesteps      | 71680        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075719045 |\n",
      "|    clip_fraction        | 0.128        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.7         |\n",
      "|    explained_variance   | -0.00048     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 47.3         |\n",
      "|    n_updates            | 830          |\n",
      "|    policy_gradient_loss | 0.00833      |\n",
      "|    value_loss           | 86.8         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 56.3        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 874         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 84          |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032925595 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.694      |\n",
      "|    explained_variance   | 0.000202    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 54.4        |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | 0.0145      |\n",
      "|    value_loss           | 98.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 55.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 875         |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 86          |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007420223 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | -5.13e-06   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 48.1        |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | 0.00992     |\n",
      "|    value_loss           | 105         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 55.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 873         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 89          |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012108208 |\n",
      "|    clip_fraction        | 0.177       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.654      |\n",
      "|    explained_variance   | 1.05e-05    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 60.4        |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | 0.0143      |\n",
      "|    value_loss           | 109         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 56.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 872         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 91          |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016398348 |\n",
      "|    clip_fraction        | 0.168       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.675      |\n",
      "|    explained_variance   | 1.19e-06    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 43.9        |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | 0.0114      |\n",
      "|    value_loss           | 111         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 57.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 874         |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 93          |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008758673 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.688      |\n",
      "|    explained_variance   | 1.55e-06    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 63.7        |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | 0.0104      |\n",
      "|    value_loss           | 111         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 56.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 873         |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 96          |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013296917 |\n",
      "|    clip_fraction        | 0.189       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.693      |\n",
      "|    explained_variance   | -1.67e-06   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 78.5        |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | 0.0139      |\n",
      "|    value_loss           | 116         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 56.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 873         |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 98          |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008546801 |\n",
      "|    clip_fraction        | 0.153       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.709      |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 55.8        |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | 0.0103      |\n",
      "|    value_loss           | 110         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 51.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 874         |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 100         |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017143644 |\n",
      "|    clip_fraction        | 0.181       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.707      |\n",
      "|    explained_variance   | 1.79e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 96.4        |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | 0.0164      |\n",
      "|    value_loss           | 116         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 60           |\n",
      "|    ep_rew_mean          | 51           |\n",
      "| time/                   |              |\n",
      "|    fps                  | 871          |\n",
      "|    iterations           | 44           |\n",
      "|    time_elapsed         | 103          |\n",
      "|    total_timesteps      | 90112        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059523545 |\n",
      "|    clip_fraction        | 0.158        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.719       |\n",
      "|    explained_variance   | 8.34e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 44.5         |\n",
      "|    n_updates            | 920          |\n",
      "|    policy_gradient_loss | 0.00483      |\n",
      "|    value_loss           | 107          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 60           |\n",
      "|    ep_rew_mean          | 50.3         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 872          |\n",
      "|    iterations           | 45           |\n",
      "|    time_elapsed         | 105          |\n",
      "|    total_timesteps      | 92160        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062444126 |\n",
      "|    clip_fraction        | 0.165        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.703       |\n",
      "|    explained_variance   | 0.000644     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 43           |\n",
      "|    n_updates            | 930          |\n",
      "|    policy_gradient_loss | 0.0114       |\n",
      "|    value_loss           | 98.9         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 50.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 873         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 107         |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009047863 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.709      |\n",
      "|    explained_variance   | 0.000558    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 59.7        |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | 0.0114      |\n",
      "|    value_loss           | 104         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 49.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 871         |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 110         |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013773875 |\n",
      "|    clip_fraction        | 0.189       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.709      |\n",
      "|    explained_variance   | 0.000462    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 47.7        |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | 0.00707     |\n",
      "|    value_loss           | 92.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 50.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 871         |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 112         |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.046037875 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.714      |\n",
      "|    explained_variance   | 0.00127     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 53.1        |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | 0.00873     |\n",
      "|    value_loss           | 90.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 55.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 874         |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 114         |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012812542 |\n",
      "|    clip_fraction        | 0.242       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.708      |\n",
      "|    explained_variance   | 0.00072     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 40          |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | 0.0208      |\n",
      "|    value_loss           | 97.2        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x1836d2f9890>"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PPO_DRL_model.learn(total_timesteps=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549fda64",
   "metadata": {},
   "source": [
    "## 4. Save PPO DRL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "9f563339",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jet Wei\\Documents\\Jetwei\\Personal Programming stuff\\Reinforcement Learning\\venv\\Lib\\site-packages\\stable_baselines3\\common\\save_util.py:284: UserWarning: Path 'Training_Project_3_Custom\\Saved RL Models' does not exist. Will create it.\n",
      "  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"
     ]
    }
   ],
   "source": [
    "PPO_Model_Custom_100k = os.path.join('Training_Project_3_Custom', 'Saved RL Models', 'PPO_Model_Custom_100k')\n",
    "PPO_DRL_model.save(PPO_Model_Custom_100k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7271d7d6",
   "metadata": {},
   "source": [
    "## 5. Reload PPO DRL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "aaf7e9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "PPO_Model_Custom_100k = os.path.join('Training_Project_3_Custom', 'Saved RL Models', 'PPO_Model_Custom_100k')\n",
    "reloaded_PPO_DRL_model = PPO.load(PPO_Model_Custom_100k, env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417b6161",
   "metadata": {},
   "source": [
    "## 6. Evaluating the PPO DRL model in a RL Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "b4a24b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(np.float64(-60.0), np.float64(0.0))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jet Wei\\Documents\\Jetwei\\Personal Programming stuff\\Reinforcement Learning\\venv\\Lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Jet Wei\\Documents\\Jetwei\\Personal Programming stuff\\Reinforcement Learning\\venv\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:259: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.\n",
      "  warnings.warn(\"You tried to call render() but no `render_mode` was passed to the env constructor.\")\n"
     ]
    }
   ],
   "source": [
    "# The 'evaluate_policy()' function returns a tuple,\n",
    "#       (mean_reward, std_reward)\n",
    "# - 'mean_reward' - refers to the mean reward throughout the episodes\n",
    "# - 'std_reward' - refers to the standard deviation of the reward throughout the episodes\n",
    "print(evaluate_policy(reloaded_PPO_DRL_model, env, n_eval_episodes=1, render=True))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba56faf",
   "metadata": {},
   "source": [
    "## 7. Test the PPO DRL model in a RL Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a7aa91",
   "metadata": {},
   "source": [
    "To test the PPO DRL model in the Gymnasium's Custom RL Environment, we can use the same code from the earlier section '2. Load RL Environment and testing if it works with a baseline algorithm that takes random actions' with some minor changes\n",
    "\n",
    "But here, instead of taking a random action at each time step in an episode, we are using the PPO DRL model to predict that action at each time step in an episode instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "14cfdc5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State: [38.]\n",
      "Episode: 0 Score: 54\n",
      "Initial State: [36.]\n",
      "Episode: 1 Score: 56\n",
      "Initial State: [38.]\n",
      "Episode: 2 Score: 58\n",
      "Initial State: [38.]\n",
      "Episode: 3 Score: 56\n",
      "Initial State: [39.]\n",
      "Episode: 4 Score: 58\n",
      "Initial State: [39.]\n",
      "Episode: 5 Score: 52\n"
     ]
    }
   ],
   "source": [
    "env = ShowerEnv()\n",
    "\n",
    "episodes = 5\n",
    "for episode in range(0, episodes+1):\n",
    "    # Initialise starting state of the RL agent in the RL Environment before an episode, done to false, and starting \n",
    "    # episode score to 0\n",
    "    obs, _ = env.reset()\n",
    "    print(f\"Initial State: {obs}\")\n",
    "    done = False\n",
    "    episode_score = 0\n",
    "\n",
    "    # During an episode:\n",
    "    while not done:\n",
    "        env.render()\n",
    "        # RL agent determines action to take\n",
    "        # - Now, we are no longer randomly sampling an action to take by our RL agent in the RL Environment, but\n",
    "        #   instead we are using the PPO DRL model to predict the action at each time step in an episode instead based\n",
    "        #   on the current observations/states in the RL Environment\n",
    "        action, _ = reloaded_PPO_DRL_model.predict(obs)\n",
    "        # RL Environment generates the next state and reward gained upon taking the action in the current state\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        # Append the reward gained upon taking the action in the current state to the cumulative episode date\n",
    "        episode_score += reward\n",
    "\n",
    "    print(f\"Episode: {episode} Score: {episode_score}\")\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
