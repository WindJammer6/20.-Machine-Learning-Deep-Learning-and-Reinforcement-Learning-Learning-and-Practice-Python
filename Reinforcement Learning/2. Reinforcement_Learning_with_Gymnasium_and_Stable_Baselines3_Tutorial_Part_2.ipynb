{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07f37ed0",
   "metadata": {},
   "source": [
    "# Reinforcement Learning (RL) with Gymnasium and Stable Baselines3 Tutorial (Part 2)\n",
    "Source: \n",
    "- https://www.youtube.com/watch?v=Mut_u40Sqz4&t=6144s (Nicholas Renotte) (YouTube video by Nicholas Renotte titled, \n",
    "'Reinforcement Learning in 3 Hours | Full Course using Python')\n",
    "\n",
    "Documentations:\n",
    "- Gymnasium: https://gymnasium.farama.org/ (This library provides standardized environments for developing and testing RL algorithms)\n",
    "- Stable Baselines3: https://stable-baselines3.readthedocs.io/en/master/guide/quickstart.html (This library provides a suite of pre-implemented RL algorithms based on PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b9b41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8000e91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_name = \"CartPole-v1\"\n",
    "env = gym.make(environment_name, render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a890dea4",
   "metadata": {},
   "source": [
    "## 5. Reload PPO DRL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6537c2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "PPO_Model_CartPole_v1_20k = os.path.join('Training', 'Saved RL Models', 'PPO_Model_CartPole_v1_20k')\n",
    "PPO_DRL_model = PPO.load(PPO_Model_CartPole_v1_20k, env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed100379",
   "metadata": {},
   "source": [
    "## 6. Evaluating the PPO DRL model in a RL Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cd7813",
   "metadata": {},
   "source": [
    "Now we can evaluate the performance of the PPO DRL model in the Gymnasium's 'CartPole-v1' RL Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce7f47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jet Wei\\Documents\\Jetwei\\Personal Programming stuff\\Reinforcement Learning\\venv\\Lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(np.float64(500.0), np.float64(0.0))\n"
     ]
    }
   ],
   "source": [
    "eval_env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "\n",
    "# The 'evaluate_policy()' function returns a tuple,\n",
    "#       (mean_reward, std_reward)\n",
    "# - 'mean_reward' - refers to the mean reward throughout the episodes\n",
    "# - 'std_reward' - refers to the standard deviation of the reward throughout the episodes\n",
    "print(evaluate_policy(PPO_DRL_model, eval_env, n_eval_episodes=1, render=True))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f7e536",
   "metadata": {},
   "source": [
    "## 7. Test the PPO DRL model in a RL Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137daec8",
   "metadata": {},
   "source": [
    "To test the PPO DRL model in the Gymnasium's 'CartPole-v1' RL Environment, we can use the same code from the earlier  section '2. Load RL Environment and testing if it works with a baseline algorithm that takes random actions' with some minor changes\n",
    "\n",
    "But here, instead of taking a random action at each time step in an episode, we are using the PPO DRL model to predict that action at each time step in an episode instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688623c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State: [ 0.01689493 -0.04276498  0.03179216  0.0038821 ]\n",
      "Episode: 0 Score: 376.0\n",
      "Initial State: [ 0.04157403  0.03359015  0.01806775 -0.02898854]\n",
      "Episode: 1 Score: 378.0\n",
      "Initial State: [ 0.00292343 -0.01035426 -0.03587482  0.04994534]\n",
      "Episode: 2 Score: 366.0\n",
      "Initial State: [ 0.0451983  -0.02441233  0.04365429 -0.03893877]\n",
      "Episode: 3 Score: 237.0\n",
      "Initial State: [ 0.03165805 -0.02900014 -0.00288191 -0.01140878]\n",
      "Episode: 4 Score: 358.0\n",
      "Initial State: [0.04184088 0.04438623 0.02803899 0.00377379]\n",
      "Episode: 5 Score: 254.0\n"
     ]
    }
   ],
   "source": [
    "# We simply need to change a few things here from the earlier section '2. Load RL Environment and testing if it works \n",
    "# with a baseline algorithm that takes random actions':\n",
    "# 1. Change the line 'state = env.reset()' and 'print(f\"Initial State: {state}\")' -> 'obs = env.reset()' and 'print(f\"Initial State: {obs}\")'\n",
    "\n",
    "# 2. Change the line 'action = env.action_space.sample()' -> 'action, _ = PPO_RL_model.predict(obs)'\n",
    "\n",
    "# 3. Change the line 'n_state, reward, done, truncated, info = env.step(action)' -> 'obs, reward, done, truncated, info = env.step(action)'\n",
    "\n",
    "environment_name = \"CartPole-v1\"\n",
    "env = gym.make(environment_name, render_mode=\"human\")\n",
    "\n",
    "episodes = 5\n",
    "for episode in range(0, episodes+1):\n",
    "    # Initialise starting state of the RL agent in the RL Environment before an episode, done to false, and starting \n",
    "    # episode score to 0\n",
    "    obs, _ = env.reset()\n",
    "    print(f\"Initial State: {obs}\")\n",
    "    done = False\n",
    "    episode_score = 0\n",
    "\n",
    "    # During an episode:\n",
    "    while not done:\n",
    "        env.render()\n",
    "        # RL agent determines action to take\n",
    "        # - Now, we are no longer randomly sampling an action to take by our RL agent in the RL Environment, but\n",
    "        #   instead we are using the PPO DRL model to predict the action at each time step in an episode instead based\n",
    "        #   on the current observations/states in the RL Environment\n",
    "        action, _ = PPO_DRL_model.predict(obs)\n",
    "        # RL Environment generates the next state and reward gained upon taking the action in the current state\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        # Append the reward gained upon taking the action in the current state to the cumulative episode date\n",
    "        episode_score += reward\n",
    "\n",
    "    print(f\"Episode: {episode} Score: {episode_score}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7658a3",
   "metadata": {},
   "source": [
    "## 8. Logging the training process of the PPO DRL model in a RL Environment in TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe35fd91",
   "metadata": {},
   "source": [
    "(Copy-pasted from the '2. training_a_Feedforward_Neural_Network_with_digits_image_dataset_with_TensorBoard.ipynb' file from the 'Tutorial 8 - TensorBoard Introduction' folder from the 'Deep Learning' tutorials)\n",
    "\n",
    "You need to:\n",
    "1. Navigate to the folder storing the 'logs' folder\n",
    "\n",
    "2. Run the following command to open the TensorBoard tool:\n",
    "    ```\n",
    "    tensorboard --logdir logs\n",
    "    ```\n",
    "\n",
    "Expected output:\n",
    "```\n",
    "2025-06-12 18:53:36.667376: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
    "2025-06-12 18:53:38.357404: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
    "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
    "TensorBoard 2.19.0 at http://localhost:6006/ (Press CTRL+C to quit)\n",
    "```\n",
    "\n",
    "Open the URL in your browser to view the TensorBoard tool:\n",
    "```\n",
    "http://localhost:6006/\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b48f0ac",
   "metadata": {},
   "source": [
    "## 9. Adding a Callback during training of the PPO DRL model in a RL Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b94107",
   "metadata": {},
   "source": [
    "### What is a Callback?\n",
    "A callback is a custom piece of code that runs at specific points during training of the RL algorithm — like at the end of an episode, after a certain number of steps, or when saving models. It gives you ability to extend/add more functionalities to help you track, control, or modify the training process of the RL algorithms while it’s running.\n",
    "\n",
    "**Here are some existing built-in callback classes in Stable Baselines3 you can use, which supports different functionalities:**\n",
    "| Callback                           | Purpose                                                                                         |\n",
    "| ---------------------------------- | ----------------------------------------------------------------------------------------------- |\n",
    "| `BaseCallback`                     | Abstract base class. All callbacks must inherit from this.                                      |\n",
    "| `EventCallback`                    | Base class for callbacks that trigger on a specific event (e.g. best model saving, evaluation). |\n",
    "| `CallbackList`                     | Combines multiple callbacks into one.                                                           |\n",
    "| `CheckpointCallback`               | Saves the model periodically during training.                                                   |\n",
    "| `EvalCallback`                     | Evaluates the agent periodically and optionally saves the best-performing model.                |\n",
    "| `StopTrainingOnRewardThreshold`    | Stops training once a certain reward threshold is reached.                                      |\n",
    "| `StopTrainingOnMaxEpisodes`        | Stops training after a set number of episodes.                                                  |\n",
    "| `StopTrainingOnNoModelImprovement` | Stops training if the model performance doesn't improve for a certain number of evaluations.    |\n",
    "| `ProgressBarCallback`              | Shows a tqdm progress bar for training.                                                         |\n",
    "| `TensorboardCallback`              | Custom callback for advanced Tensorboard logging (often user-defined).                          |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02758c87",
   "metadata": {},
   "source": [
    "In this tutorial hoowever, we will only be looking at the 'EvalCallback' and 'StopTrainingOnRewadThreshold' callback classes, which, used together, allows us to stop training the RL algorithm after it has reached a certain reward threshold in a certain episode. Then, it will save the RL algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6fecd8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50f2cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join('Training_Tutorial', 'Saved RL Models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "76ca1973",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_name = \"CartPole-v1\"\n",
    "env = gym.make(environment_name, render_mode=\"human\")\n",
    "env = DummyVecEnv([lambda: env])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4337ee8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 'StopTrainingOnRewardThreshold' callback class defines which reward threshold to stop the training of the RL algorithm\n",
    "stop_callback = StopTrainingOnRewardThreshold(reward_threshold=200, verbose=1)\n",
    "\n",
    "# The 'EvalCallback' callback class defines how often to check the defined callback, which is the \n",
    "# 'StopTrainingOnRewardThreshold' callback class in this case.\n",
    "\n",
    "# The 5 more important parameters of the 'EvalCallback' callback class are:\n",
    "# - 'env'                   - the RL environment\n",
    "# - 'callback_on_new_best'  - the callback class to execute\n",
    "# - 'eval_freq'             - how often to execute the callback class in 'callback_on_new_best'\n",
    "# - 'best_model_save_path'  - which path to save the RL model whenever it achieves a new best mean reward during evaluation\n",
    "# - 'verbose'               - controls how much information is printed to the console/log during training\n",
    "#                             -> 'verbose=0' means 'Silent', no output at all\n",
    "#                             -> 'verbose=1' means 'Info', shows key training events: episode rewards, updates, losses, etc.\n",
    "#                             -> 'verbose=2' means 'Debug' shows more detailed info like hyperparameters, rollout steps, and internal logs\n",
    "\n",
    "# The best RL model will be saved automatically in a file named 'best_model.zip'\n",
    "eval_callback = EvalCallback(env,\n",
    "                             callback_on_new_best=stop_callback,\n",
    "                             eval_freq=10000,\n",
    "                             best_model_save_path=save_path,\n",
    "                             verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0125f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "log_path = os.path.join('Training_Tutorial', 'logs')\n",
    "another_PPO_DRL_model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a599bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\logs\\PPO_3\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 47   |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 43   |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 46          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 87          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008842143 |\n",
      "|    clip_fraction        | 0.068       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.668      |\n",
      "|    explained_variance   | 0.109       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.4        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0201     |\n",
      "|    value_loss           | 41.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5904, episode_reward=429.60 +/- 97.94\n",
      "Episode length: 429.60 +/- 97.94\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 430         |\n",
      "|    mean_reward          | 430         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5904        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011382322 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.632      |\n",
      "|    explained_variance   | 0.201       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 25.9        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0195     |\n",
      "|    value_loss           | 59.9        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Stopping training because the mean reward 429.60  is above the threshold 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x1beb74365d0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "another_PPO_DRL_model.learn(total_timesteps=20000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7d6aef",
   "metadata": {},
   "source": [
    "## 10. Changing PPO DRL model policies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b58800",
   "metadata": {},
   "source": [
    "**Policy** refers to the 'learning' architecture of RL algorithms defines how the agent chooses actions based on its current state.\n",
    "\n",
    "For PPO DRL algorithm, which is a Deep RL (DRL) algorithm, it uses neural networks or its variants as its 'learning' architecture.\n",
    "\n",
    "In Stable Baseliens3, you can change the underlying policy/neural network variant used in the RL algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8092297",
   "metadata": {},
   "source": [
    "### What does this mean?\n",
    "```python\n",
    "[dict(pi=[128,128,128,128], vf=[128,128,128,128])]\n",
    "```\n",
    "\n",
    "Within Deep RL (DRL) algorithms, they have their own subcategory of 'learning' architectures/policy, despite all of them using neural networks, some using multiple neural networks:\n",
    "| **DRL Algorithm**    | **Networks Used**                             | **# of Neural Networks**                         | **Purpose of Each Network**                   |\n",
    "| ---------------- | --------------------------------------------- | ------------------------------------ | --------------------------------------------- |\n",
    "| **DQN**          | Q-Network (+ Target Q-Network)                | 1 (conceptually 1; target is a copy) | Q(s, a) for all actions                       |                  |\n",
    "| **HER**          | Q-Network (with goal relabeling)              | 1                                    | Goal-conditioned Q(s, a)                      |                  |\n",
    "| **PPO**          | Actor + Critic networks                       | 2                                    | Actor: π(a \\| s), Critic: V(s) |\n",
    "| **A2C / A3C**    | Actor + Critic networks                       | 2                                    | Actor: π(a \\| s), Critic: V(s) |\n",
    "| **TRPO**         | Actor + Critic networks                       | 2                                    | Trust-region actor + baseline value function  |\n",
    "| **DDPG**         | Actor + Critic (+ Target copies)              | 2                                    | Actor: μ(s), Critic: Q(s, a)                  |\n",
    "| **TD3**          | Actor + 2 Critics (+ Targets)                 | 3                                    | Double critics to reduce overestimation       |\n",
    "| **SAC**          | Stochastic Actor + 2 Critics + Entropy Critic | 3+                                   | Soft policy + value critics                   |\n",
    "\n",
    "Hence, for PPO DRL algorithm, the specified new policy neural network 'learning' architecture,\n",
    "```python\n",
    "[dict(pi=[128,128,128,128], vf=[128,128,128,128])]\n",
    "```\n",
    "means that:\n",
    "- 'pi' defines the architecture for the policy network (the actor that outputs action probabilities).\n",
    "- 'vf' defines the architecture for the value function network (the critic that estimates the value of a state). \n",
    "\n",
    "| Network       | Layers                                | Description                                                          |\n",
    "| ------------- | ------------------------------------- | -------------------------------------------------------------------- |\n",
    "| Policy (`pi`) | 4 hidden layers with 128 neurons each | Used to decide what action to take                                   |\n",
    "| Value (`vf`)  | 4 hidden layers with 128 neurons each | Used to estimate how good a state is (used in advantage calculation) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2483f115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "new_policy_neural_network_learning_architecture = [dict(pi=[128,128,128,128], vf=[128,128,128,128])]\n",
    "another_another_PPO_DRL_model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path, policy_kwargs={'net_arch' : new_policy_neural_network_learning_architecture})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58913c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\logs\\PPO_4\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 46   |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 44   |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "--------------------------------------\n",
      "| time/                   |          |\n",
      "|    fps                  | 45       |\n",
      "|    iterations           | 2        |\n",
      "|    time_elapsed         | 89       |\n",
      "|    total_timesteps      | 4096     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.014807 |\n",
      "|    clip_fraction        | 0.196    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.682   |\n",
      "|    explained_variance   | 6.68e-06 |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | 4.04     |\n",
      "|    n_updates            | 10       |\n",
      "|    policy_gradient_loss | -0.0227  |\n",
      "|    value_loss           | 21.9     |\n",
      "--------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 45          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 134         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015298017 |\n",
      "|    clip_fraction        | 0.198       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.639      |\n",
      "|    explained_variance   | 0.433       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.33        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0328     |\n",
      "|    value_loss           | 26.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 45          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 178         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013342932 |\n",
      "|    clip_fraction        | 0.177       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.607      |\n",
      "|    explained_variance   | 0.551       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.45        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0284     |\n",
      "|    value_loss           | 34.4        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jet Wei\\Documents\\Jetwei\\Personal Programming stuff\\Reinforcement Learning\\venv\\Lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=10000, episode_reward=349.60 +/- 84.60\n",
      "Episode length: 349.60 +/- 84.60\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 350          |\n",
      "|    mean_reward          | 350          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 10000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0113417385 |\n",
      "|    clip_fraction        | 0.126        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.577       |\n",
      "|    explained_variance   | 0.596        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 10.1         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0189      |\n",
      "|    value_loss           | 39.4         |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 39    |\n",
      "|    iterations      | 5     |\n",
      "|    time_elapsed    | 259   |\n",
      "|    total_timesteps | 10240 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 40          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 304         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012441041 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.561      |\n",
      "|    explained_variance   | 0.715       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.5        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0112     |\n",
      "|    value_loss           | 24.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 40          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 350         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011519846 |\n",
      "|    clip_fraction        | 0.189       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.552      |\n",
      "|    explained_variance   | 0.882       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.71        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0141     |\n",
      "|    value_loss           | 17.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 41           |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 397          |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0101861805 |\n",
      "|    clip_fraction        | 0.105        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.53        |\n",
      "|    explained_variance   | 0.907        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.933        |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00755     |\n",
      "|    value_loss           | 8.11         |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 41         |\n",
      "|    iterations           | 9          |\n",
      "|    time_elapsed         | 443        |\n",
      "|    total_timesteps      | 18432      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01283386 |\n",
      "|    clip_fraction        | 0.153      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.516     |\n",
      "|    explained_variance   | 0.968      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.756      |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.013     |\n",
      "|    value_loss           | 4.29       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 500       |\n",
      "|    mean_reward          | 500       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 20000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0116706 |\n",
      "|    clip_fraction        | 0.129     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.49     |\n",
      "|    explained_variance   | 0.95      |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.359     |\n",
      "|    n_updates            | 90        |\n",
      "|    policy_gradient_loss | -0.00511  |\n",
      "|    value_loss           | 2.7       |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "Stopping training because the mean reward 500.00  is above the threshold 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x1beb74c3010>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "another_another_PPO_DRL_model.learn(total_timesteps=20000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f54cc8",
   "metadata": {},
   "source": [
    "## 11. Using an alternate DRL algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab181659",
   "metadata": {},
   "source": [
    "In Stable Baselines3, it comes with pre-packaged a number of different RL algorithms that you can use.\n",
    "\n",
    "From the Stable Baselines3 documentation: https://stable-baselines3.readthedocs.io/en/master/guide/quickstart.html\n",
    "\n",
    "These are the pre-packaged RL algorithms in the Stable Baselines3 library:\n",
    "| **RL Algorithm** | **Full Name**                                                          | **Category**                                                   |\n",
    "| ---------------- | ---------------------------------------------------------------------- | -------------------------------------------------------------- |\n",
    "| **A2C**          | Advantage Actor-Critic                                                 | On-policy, Actor-Critic                                        |\n",
    "| **ACER**         | Actor-Critic with Experience Replay                                    | Off-policy, Actor-Critic                                       |\n",
    "| **ACKTR**        | Actor-Critic using Kronecker-Factored Trust Region                     | On-policy, Actor-Critic                                        |\n",
    "| **DDPG**         | Deep Deterministic Policy Gradient                                     | Off-policy, Actor-Critic (deterministic)                       |\n",
    "| **DQN**          | Deep Q-Network                                                         | Off-policy, Value-based                                        |\n",
    "| **GAIL**         | Generative Adversarial Imitation Learning                              | Imitation Learning                                             |\n",
    "| **HER**          | Hindsight Experience Replay                                            | Experience replay strategy for goal-based learning             |\n",
    "| **PPO1**         | Proximal Policy Optimization (Original/OpenAI Baseline implementation) | On-policy, Actor-Critic                                        |\n",
    "| **PPO2**         | Proximal Policy Optimization (Improved version from OpenAI Baselines)  | On-policy, Actor-Critic                                        |\n",
    "| **SAC**          | Soft Actor-Critic                                                      | Off-policy, Actor-Critic (stochastic + entropy regularization) |\n",
    "| **TD3**          | Twin Delayed Deep Deterministic Policy Gradient                        | Off-policy, Actor-Critic (improvement over DDPG)               |\n",
    "| **TRPO**         | Trust Region Policy Optimization                                       | On-policy, Actor-Critic                                        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f497fa87",
   "metadata": {},
   "source": [
    "Just for demonstration purposes, the alternate RL algorithm that we will be using here is the Deep Q-Network (DQN) DRL algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8092d046",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be331ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "DQN_DRL_model = DQN('MlpPolicy', env, verbose=20000, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed418a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\logs\\DQN_1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x1beb7462010>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DQN_DRL_model.learn(total_timesteps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6a855c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DQN_Model_CartPole_v1_20k = os.path.join('Training_Tutorial', 'Saved RL Models', 'DQN_Model_CartPole_v1_20k')\n",
    "DQN_DRL_model.save(DQN_Model_CartPole_v1_20k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
