{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07f37ed0",
   "metadata": {},
   "source": [
    "# Reinforcement Learning (RL) with Gymnasium and Stable Baselines3 Tutorial\n",
    "Source: \n",
    "- https://www.youtube.com/watch?v=Mut_u40Sqz4&t=6144s (Nicholas Renotte) (YouTube video by Nicholas Renotte titled, \n",
    "'Reinforcement Learning in 3 Hours | Full Course using Python')\n",
    "\n",
    "Documentations:\n",
    "- Gymnasium: https://gymnasium.farama.org/ (This library provides standardized environments for developing and testing RL algorithms)\n",
    "- Stable Baselines3: https://stable-baselines3.readthedocs.io/en/master/guide/quickstart.html (This library provides a suite of pre-implemented RL algorithms based on PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db8d3f8",
   "metadata": {},
   "source": [
    "# Project 1: Breakout RL Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3fc1b7",
   "metadata": {},
   "source": [
    "### What are Atari Games RL Environments?\n",
    "In Gymansium, there is a class of RL Environments called Atari Games, which refers to the classic video games from the Atari 2600 console, such as:\n",
    "- Breakout\n",
    "- Pong\n",
    "- Space Invaders\n",
    "- Q*Bert\n",
    "- Seaquest\n",
    "- Montezuma's Revenge\n",
    "\n",
    "and many more...\n",
    "\n",
    "These games are used as benchmark RL Environments for evaluating and comparing the performance of RL algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12377cc4",
   "metadata": {},
   "source": [
    "## How is an RL Environment defined?\n",
    "An RL Environment is typically modeled as the 5-tuple:\n",
    "```text\n",
    "ùëÄ=(ùëÜ,ùê¥,ùëÉ,ùëÖ,ùõæ)\n",
    "```\n",
    "\n",
    "An RL Environment is defined as 5-tuple in the framework of a Markov Decision Process (MDP):\n",
    "\n",
    "| Symbol              | Name                       | Description                                                                               |\n",
    "| ------------------- | -------------------------- | ----------------------------------------------------------------------------------------- |\n",
    "| $S$                 | **States**                 | The set of all possible states the agent can be in                                        |\n",
    "| $A$                 | **Actions**                | The set of all possible actions the agent can take                                        |\n",
    "| $P(s' \\mid s, a)$   | **Transition Probability** | The probability of moving to state $s'$ after taking action $a$ in state $s$              |\n",
    "| $R(s, a)$           | **Reward Function**        | The expected reward received after taking action $a$ in state $s$                         |\n",
    "| $\\gamma \\in [0, 1]$ | **Discount Factor**        | The factor by which future rewards are discounted (controls how far-sighted the agent is) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5c3a14",
   "metadata": {},
   "source": [
    "## How does Gymnasium represent each of these components of the RL Environment?\n",
    "**States**/**Observations** and  **Actions**  \n",
    "- Box ‚Äì n-dimensional tensor, range of values (continuous values)\n",
    "    ```\n",
    "    E.g. Box(0, 1, shape=(3,3))\n",
    "    ```\n",
    "- Discrete ‚Äì Set of items (discrete values)\n",
    "    ```\n",
    "    E.g. Discrete(3)\n",
    "    ```\n",
    "- Tuple ‚Äì Tuple of other spaces (e.g., Box or Discrete)\n",
    "    ```\n",
    "    E.g. Tuple((Discrete(2), Box(0, 100, shape=(1,))))\n",
    "    ```\n",
    "- Dict ‚Äì Dictionary of spaces (e.g., Box or Discrete)\n",
    "    ```\n",
    "    E.g. Dict({\"height\": Discrete(2), \"speed\": Box(0, 100, shape=(1,))})\n",
    "    ```\n",
    "- MultiBinary ‚Äì One-hot encoded binary values\n",
    "    ```\n",
    "    E.g. MultiBinary(4)\n",
    "    ```\n",
    "- MultiDiscrete ‚Äì Multiple discrete values\n",
    "    ```\n",
    "    E.g. MultiDiscrete([5, 2, 2])\n",
    "    ```\n",
    "\n",
    "**Transition Probability**  \n",
    "- abstracted out by the Gymmnasium library\n",
    "\n",
    "**Reward Function**  \n",
    "- abstracted out by the Gymmnasium library\n",
    "\n",
    "**Discount Factor**\n",
    "- abstracted out by the Gymmnasium library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5cd274",
   "metadata": {},
   "source": [
    "### What is the difference between States and Observations?\n",
    "RL agents only act on observations, not states. Optimal behavior of RL agents assumes knowledge of the underlying state (or estimates of it).\n",
    "\n",
    "| **Aspect**          | **State**                                                    | **Observation**                                                 |\n",
    "| ------------------- | ------------------------------------------------------------ | --------------------------------------------------------------- |\n",
    "| **Definition**      | The **true internal configuration** of the environment       | The **information** the agent **receives** from the environment |\n",
    "| **Completeness**    | Often assumed to be **complete** (Markov property holds)     | May be **partial**, noisy, or incomplete view of the state      |\n",
    "| **Markov Property** | A true state satisfies: future depends only on current state | Observations may not satisfy the Markov property                |\n",
    "| **Agent‚Äôs View**    | Agent may not have access to the full state                  | Agent always uses observations to decide actions                |\n",
    "| **Example**         | All object positions, velocities, and environment internals  | Camera image, radar scan, or any sensor reading                 |\n",
    "\n",
    "**MDP vs POMDP**\n",
    "- In fully observable environments (e.g., many standard RL benchmarks), the observation is equivalent to the state. This is assumed in Markov Decision Processes (MDPs).\n",
    "- In Partially Observable MDPs (POMDPs), the agent sees only observations and must infer the state using memory or belief models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46deb6f",
   "metadata": {},
   "source": [
    "## 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77af2fc7",
   "metadata": {},
   "source": [
    "**To run Gymnasium and Stable Baselines3 libraries, it is HIGHLY recommended to create a virtual environment and download the dependencies/requirements in the virtual environment seperately to prevent conflicts in libraries!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d327991e",
   "metadata": {},
   "source": [
    "### How to set up a virtual environment in VS Code?\n",
    "1. **Create a virtual environment**\n",
    "    ```bash\n",
    "    python -m venv venv\n",
    "    ```\n",
    "    This creates a folder named venv/ containing the isolated environment.\n",
    "\n",
    "2. **Activate the virtual environment**\n",
    "\n",
    "    For Windows:\n",
    "    ```bash\n",
    "    .\\venv\\Scripts\\activate\n",
    "    ```\n",
    "    For macOS/Linux:\n",
    "    ```bash\n",
    "    source venv/bin/activate\n",
    "    ```\n",
    "    You‚Äôll know it‚Äôs activated when your terminal prompt changes to show (venv).\n",
    "\n",
    "3. **Now you can install dependencies inside the virtual environment!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26538f5a",
   "metadata": {},
   "source": [
    "### What dependencies/requirements to download? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649db53c",
   "metadata": {},
   "source": [
    "**For Gymnasium library**\n",
    "```bash\n",
    "pip install gymnasium\n",
    "```\n",
    "\n",
    "**For Stable Baselines3 library**\n",
    "```bash\n",
    "pip install stable-baselines3[extra]\n",
    "```\n",
    "\n",
    "**For ALE (Arcade Learning Environment) package**  \n",
    "The current newer version of Gymnasium library no longer include Atari Games RL Environments anymore by default. To use these Atari Games RL Environments with Gymnasium, you need to download a seperate dependency/package, the ALE (Arcade Learning Environment) package.\n",
    "```bash\n",
    "pip install autorom[accept-rom-license]\n",
    "pip install ale-py\n",
    "```\n",
    "\n",
    "Source(s):\n",
    "- https://github.com/AndreM96/Stable_Baseline3_Gymnasium_Tutorial (AndreM96 on Github)\n",
    "- https://www.youtube.com/watch?v=Mut_u40Sqz4&t=6144s (one of the comments under the YouTube video by Nicholas Renotte titled, 'Reinforcement Learning in 3 Hours | Full Course using Python')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7bbf80",
   "metadata": {},
   "source": [
    "Just for demonstration purposes, the RL algorithm that we will be using here is the Advantage Actor-Critic (A2C) DRL algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "80b9b41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "from ale_py import ALEInterface\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_atari_env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a841db34",
   "metadata": {},
   "source": [
    "## 2. Load RL Environment and testing if it works with a baseline algorithm that takes random actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f13962",
   "metadata": {},
   "source": [
    "Just for demonstration purposes, the RL Environment that we will be using here is the \"Breakout-v0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bccdc18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State: [[[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]]\n",
      "Episode: 0 Score: 2.0\n",
      "Initial State: [[[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]]\n",
      "Episode: 1 Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "environment_name = \"Breakout-v0\"\n",
    "env = gym.make(environment_name, render_mode=\"human\")\n",
    "\n",
    "episodes = 1\n",
    "for episode in range(0, episodes+1):\n",
    "    # Initialise starting state of the RL agent in the RL Environment before an episode, done to false, and starting \n",
    "    # episode score to 0\n",
    "    obs, _ = env.reset()\n",
    "    print(f\"Initial State: {obs}\")\n",
    "    done = False\n",
    "    episode_score = 0\n",
    "\n",
    "    # During an episode:\n",
    "    while not done:\n",
    "        env.render()\n",
    "        # RL agent determines action to take\n",
    "        # - In this case, we are randomly sampling an action to take by our RL agent in the RL Environment (this line of\n",
    "        #   code defines that baseline algorithm that takes random actions (instead of an RL algorithm))\n",
    "        action = env.action_space.sample()\n",
    "        # RL Environment generates the next state and reward gained upon taking the action in the current state\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        # Append the reward gained upon taking the action in the current state to the cumulative episode date\n",
    "        episode_score += reward\n",
    "\n",
    "    print(f\"Episode: {episode} Score: {episode_score}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d356cd6",
   "metadata": {},
   "source": [
    "### Understanding the RL Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d2c658",
   "metadata": {},
   "source": [
    "From the Gymansium's \"Breakout-v0\" RL Environment (image version, there is another version which is the RAM version) documentation: https://ale.farama.org/environments/breakout/#\n",
    "\n",
    "**States**  \n",
    "Type: Box(0, 255, (210, 160, 3))\n",
    "| Num | Observation      | Min | Max | Description                        |\n",
    "| --- | ---------------- | --- | --- | ---------------------------------- |\n",
    "| 0   | RGB image frame  | 0   | 255 | Raw screen image (pixel intensity) |\n",
    "\n",
    "\n",
    "**Actions**  \n",
    "Type: Discrete(18)\n",
    "| **Index** | **Action Name** | **Meaning**            |\n",
    "| --------- | --------------- | ---------------------- |\n",
    "| 0         | NOOP            | Do nothing             |\n",
    "| 1         | FIRE            | Press fire button only |\n",
    "| 2         | UP              | Move joystick up       |\n",
    "| 3         | RIGHT           | Move right             |\n",
    "| 4         | LEFT            | Move left              |\n",
    "| 5         | DOWN            | Move down              |\n",
    "| 6         | UPRIGHT         | UP + RIGHT             |\n",
    "| 7         | UPLEFT          | UP + LEFT              |\n",
    "| 8         | DOWNRIGHT       | DOWN + RIGHT           |\n",
    "| 9         | DOWNLEFT        | DOWN + LEFT            |\n",
    "| 10        | UPFIRE          | UP + FIRE              |\n",
    "| 11        | RIGHTFIRE       | RIGHT + FIRE           |\n",
    "| 12        | LEFTFIRE        | LEFT + FIRE            |\n",
    "| 13        | DOWNFIRE        | DOWN + FIRE            |\n",
    "| 14        | UPRIGHTFIRE     | UP + RIGHT + FIRE      |\n",
    "| 15        | UPLEFTFIRE      | UP + LEFT + FIRE       |\n",
    "| 16        | DOWNRIGHTFIRE   | DOWN + RIGHT + FIRE    |\n",
    "| 17        | DOWNLEFTFIRE    | DOWN + LEFT + FIRE     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "09aee170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(0, 255, (210, 160, 3), uint8)\n",
      "Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "# Understanding the state and action spaces used in the Gymnasium's \"Breakout-v0\" RL Environment\n",
    "print(env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f31e19a",
   "metadata": {},
   "source": [
    "## 3. Vectorise RL Environment and Train an A2C DRL algorithm in a RL Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3416be12",
   "metadata": {},
   "source": [
    "### What is an Reinforcement Learning (RL) algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e22779",
   "metadata": {},
   "source": [
    "An RL algorithm involves an agent performing actions in an RL environment, receiving rewards or penalties based on those actions, and adjusting its behavior accordingly. This loop helps the agent improve its decision-making over time to maximize the cumulative reward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc16bfc8",
   "metadata": {},
   "source": [
    "### How does a Reinforcement Learning (RL) algorithm 'learn'?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f55666",
   "metadata": {},
   "source": [
    "In ML and DL, we learnt that ML/DL algorithms 'learn' by updating the ML/DL algorithm's weights and biases as more datas are fed into the ML/DL algorithm, and after many iterations of training, it makes accurate predictions. \n",
    "\n",
    "**This is no different in RL.**\n",
    "\n",
    "In RL, the RL algorithms uses various architectures to 'learn' by updating the RL algorithm's weights and biases as it interacts more with the RL Environment (via the reward mechanism). The 'learning' architecture used also defines whether a RL algorithm is a **Classical RL algorithm** or a **Deep RL (DRL) algorithm**.\n",
    "\n",
    "**Classical RL algorithm learning architectures**  \n",
    "Uses tables or simple functions:\n",
    "| Type                          | Description                                                                      | Example             |\n",
    "| ----------------------------- | -------------------------------------------------------------------------------- | ------------------- |\n",
    "| **Tabular policy**            | Table stores the best action for each discrete state                             | `œÄ[s] = a`          |\n",
    "| **Tabular stochastic policy** | Table of probabilities for each action in each state                             | `œÄ[a][s] = P(a \\| s)` |\n",
    "| **Value-based methods**       | Use a value table (e.g., Q-table) and derive policy as `œÄ(s) = argmax Q(s,a)`    | Q-Learning          |\n",
    "| **Policy iteration**          | Alternates between evaluating a policy and improving it based on value estimates | Dynamic Programming |      |\n",
    "| **Function approximation**    | Uses linear models or tile coding to generalize across large state spaces        | `œÄ(s) = Œ∏·µÄœÜ(s)`     |\n",
    "\n",
    "**Deep RL (DRL) algorithm learning architectures**  \n",
    "Uses neural networks or its variants,\n",
    "- FNN/MLP\n",
    "- CNN\n",
    "- RNN\n",
    "- LSTM\n",
    "- GRU\n",
    "\n",
    "In RL, after many iterations of training, it makes accurate predictions, more specifically, it behaves better/takes better actions. \n",
    "\n",
    "These RL algorithm 'learning' architectures is also called **Policy**, which defines how the agent chooses actions based on its current state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6266f8a",
   "metadata": {},
   "source": [
    "### What does a Vectorised RL Environment mean?\n",
    "Vectorized RL Environments are RL Environments that can be made to run in parallel, allowing multiple simulations at once to increase training speed of the RL algorithm.\n",
    "\n",
    "A non-vectorized RL Environment does not allow for being made to run in parallel (only one simulation can run at a time).\n",
    "\n",
    "In Gymnasium, some RL Environments are vectorized by default (e.g. Breakout), while others are not (e.g. CartPole). But when training a RL algorithm from Stable Baselines3, it is required for the RL Environment to be vectorized as well (even if you dont intend to run them in parallel)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb7b608",
   "metadata": {},
   "source": [
    "Since the RL Environment used here is \"Breakout-v0\", which is vectorized by default, you don't need to manually vectorize them.\n",
    "\n",
    "To allow the running of multiple simulations at once to increase training speed of the RL algorithm, you can do so as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0a404bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Stable Baselines3 'make_atari_env()' helper function helps create wrapped Atari Game RL Environments\n",
    "# The 3 more important parameters are:\n",
    "# - env_id (where 'environment_name' is at) - stores the RL Environment to be used\n",
    "# - n_envs                                  - specifies the number of simulations of the RL Environment to run at once\n",
    "# - seed                                    - controls the randomness of the RL Environments and ensures that experiments \n",
    "#                                             are reproducible by keeping the same seed\n",
    "env = make_atari_env(environment_name, n_envs=4, seed=0)\n",
    "# The Stable Baselines3 'VecFrameStack' class allows you to stack the RL Environments together\n",
    "env = VecFrameStack(env, n_stack=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fccc21b",
   "metadata": {},
   "source": [
    "After vectorising and increasing the number of simulations to be run at once to increase training speed of the RL algorithm, when you run the 'reset()' and 'render()' functions of the RL Environment, you can visually see that there will be multiple simulations being created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c6c7530a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]]], shape=(420, 320, 3), dtype=uint8)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0605d69b",
   "metadata": {},
   "source": [
    "### For logging purposes of the training process of the A2C DRL algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "eca901c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training_Project_1_Breakout\\logs\n"
     ]
    }
   ],
   "source": [
    "# Stating the path where we want to store our training logs files in the local folder './Training_Project_1_Breakout/logs'\n",
    "log_path = os.path.join('Training_Project_1_Breakout', 'logs')\n",
    "print(log_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab83cefd",
   "metadata": {},
   "source": [
    "### Creating the A2C DRL algorithm in the RL Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dbbd9bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "# What does each of the parameters in the 'A2C' DRL algorithm class mean?\n",
    "# - 'policy' (e.g. 'MlpPolicy'  - refers to the learning architecture used a the policy of the RL algorithm, which in this\n",
    "#               or 'CnnPolicy')   is FNN/MLP\n",
    "# - 'env'                       - refers to the RL environment to train the RL algorithm in\n",
    "# - 'verbose'                   - controls how much information is printed to the console/log during training\n",
    "#                                 -> 'verbose=0' means 'Silent', no output at all\n",
    "#                                 -> 'verbose=1' means 'Info', shows key training events: episode rewards, updates, losses, etc.\n",
    "#                                 -> 'verbose=2' means 'Debug' shows more detailed info like hyperparameters, rollout steps, and internal logs\n",
    "# - 'tensorboard_log'           - states to do the training logging in Tensorboard\n",
    "A2C_DRL_model = A2C('CnnPolicy', env, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e1e82ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInit signature:\u001b[39m\n",
      "A2C(\n",
      "    policy: Union[str, type[stable_baselines3.common.policies.ActorCriticPolicy]],\n",
      "    env: Union[gymnasium.core.Env, ForwardRef(\u001b[33m'VecEnv'\u001b[39m), str],\n",
      "    learning_rate: Union[float, Callable[[float], float]] = \u001b[32m0.0007\u001b[39m,\n",
      "    n_steps: int = \u001b[32m5\u001b[39m,\n",
      "    gamma: float = \u001b[32m0.99\u001b[39m,\n",
      "    gae_lambda: float = \u001b[32m1.0\u001b[39m,\n",
      "    ent_coef: float = \u001b[32m0.0\u001b[39m,\n",
      "    vf_coef: float = \u001b[32m0.5\u001b[39m,\n",
      "    max_grad_norm: float = \u001b[32m0.5\u001b[39m,\n",
      "    rms_prop_eps: float = \u001b[32m1e-05\u001b[39m,\n",
      "    use_rms_prop: bool = \u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "    use_sde: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "    sde_sample_freq: int = -\u001b[32m1\u001b[39m,\n",
      "    rollout_buffer_class: Optional[type[stable_baselines3.common.buffers.RolloutBuffer]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    rollout_buffer_kwargs: Optional[dict[str, Any]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    normalize_advantage: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "    stats_window_size: int = \u001b[32m100\u001b[39m,\n",
      "    tensorboard_log: Optional[str] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    policy_kwargs: Optional[dict[str, Any]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    verbose: int = \u001b[32m0\u001b[39m,\n",
      "    seed: Optional[int] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    device: Union[torch.device, str] = \u001b[33m'auto'\u001b[39m,\n",
      "    _init_setup_model: bool = \u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      ")\n",
      "\u001b[31mDocstring:\u001b[39m     \n",
      "Advantage Actor Critic (A2C)\n",
      "\n",
      "Paper: https://arxiv.org/abs/1602.01783\n",
      "Code: This implementation borrows code from https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail and\n",
      "and Stable Baselines (https://github.com/hill-a/stable-baselines)\n",
      "\n",
      "Introduction to A2C: https://hackernoon.com/intuitive-rl-intro-to-advantage-actor-critic-a2c-4ff545978752\n",
      "\n",
      ":param policy: The policy model to use (MlpPolicy, CnnPolicy, ...)\n",
      ":param env: The environment to learn from (if registered in Gym, can be str)\n",
      ":param learning_rate: The learning rate, it can be a function\n",
      "    of the current progress remaining (from 1 to 0)\n",
      ":param n_steps: The number of steps to run for each environment per update\n",
      "    (i.e. batch size is n_steps * n_env where n_env is number of environment copies running in parallel)\n",
      ":param gamma: Discount factor\n",
      ":param gae_lambda: Factor for trade-off of bias vs variance for Generalized Advantage Estimator.\n",
      "    Equivalent to classic advantage when set to 1.\n",
      ":param ent_coef: Entropy coefficient for the loss calculation\n",
      ":param vf_coef: Value function coefficient for the loss calculation\n",
      ":param max_grad_norm: The maximum value for the gradient clipping\n",
      ":param rms_prop_eps: RMSProp epsilon. It stabilizes square root computation in denominator\n",
      "    of RMSProp update\n",
      ":param use_rms_prop: Whether to use RMSprop (default) or Adam as optimizer\n",
      ":param use_sde: Whether to use generalized State Dependent Exploration (gSDE)\n",
      "    instead of action noise exploration (default: False)\n",
      ":param sde_sample_freq: Sample a new noise matrix every n steps when using gSDE\n",
      "    Default: -1 (only sample at the beginning of the rollout)\n",
      ":param rollout_buffer_class: Rollout buffer class to use. If ``None``, it will be automatically selected.\n",
      ":param rollout_buffer_kwargs: Keyword arguments to pass to the rollout buffer on creation.\n",
      ":param normalize_advantage: Whether to normalize or not the advantage\n",
      ":param stats_window_size: Window size for the rollout logging, specifying the number of episodes to average\n",
      "    the reported success rate, mean episode length, and mean reward over\n",
      ":param tensorboard_log: the log location for tensorboard (if None, no logging)\n",
      ":param policy_kwargs: additional arguments to be passed to the policy on creation. See :ref:`a2c_policies`\n",
      ":param verbose: Verbosity level: 0 for no output, 1 for info messages (such as device or wrappers used), 2 for\n",
      "    debug messages\n",
      ":param seed: Seed for the pseudo random generators\n",
      ":param device: Device (cpu, cuda, ...) on which the code should be run.\n",
      "    Setting it to auto, the code will be run on the GPU if possible.\n",
      ":param _init_setup_model: Whether or not to build the network at the creation of the instance\n",
      "\u001b[31mFile:\u001b[39m           c:\\users\\jet wei\\documents\\jetwei\\personal programming stuff\\reinforcement learning\\venv\\lib\\site-packages\\stable_baselines3\\a2c\\a2c.py\n",
      "\u001b[31mType:\u001b[39m           ABCMeta\n",
      "\u001b[31mSubclasses:\u001b[39m     "
     ]
    }
   ],
   "source": [
    "A2C?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba45b84",
   "metadata": {},
   "source": [
    "### Training the A2C DRL algorithm in the RL Environment to become a A2C DRL model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cd8155",
   "metadata": {},
   "source": [
    "Note that the number of timesteps/iterations/episodes to be used here to train an RL algorithm varies depending on the complexity of the RL Environment.\n",
    "\n",
    "For this tutorial's RL Environment, 'Breakout-v0', it is moderately complex and should take about 100 000 to 200 000 timesteps/iterations/episodes compared to the simpler 'CartPole-v1' RL Environment which should only take about 20 000 timesteps/iterations/episodes, but for more complex RL Environments it may take up to 500 000 timesteps/iterations/episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5824d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "A2C_DRL_model.learn(total_timesteps=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549fda64",
   "metadata": {},
   "source": [
    "## 4. Save A2C DRL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f563339",
   "metadata": {},
   "outputs": [],
   "source": [
    "A2C_Model_Breakout_v0_100k = os.path.join('Training_Project_1_Breakout', 'Saved RL Models', 'A2C_Model_Breakout_v0_100k')\n",
    "A2C_DRL_model.save(A2C_Model_Breakout_v0_100k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7271d7d6",
   "metadata": {},
   "source": [
    "## 5. Reload A2C DRL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "aaf7e9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "A2C_Model_Breakout_v0_100k = os.path.join('Training_Project_1_Breakout', 'Saved RL Models', 'A2C_Model_Breakout_v0_100k')\n",
    "reloaded_A2C_DRL_model = A2C.load(A2C_Model_Breakout_v0_100k, env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417b6161",
   "metadata": {},
   "source": [
    "## 6. Evaluating the A2C DRL model in a RL Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b4a24b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(np.float64(2.0), np.float64(0.0))\n"
     ]
    }
   ],
   "source": [
    "# Recall that previously you vectorised by the 'Breakout-v0' RL Environment into running multiple simulations in \n",
    "# parallel. Hence, now you need to revert it back to only running 1 simulation sequentially so that your Gymnasium\n",
    "# 'evaluate_policy()' function can work.\n",
    "eval_env = make_atari_env('Breakout-v0', n_envs=1, seed=0)\n",
    "eval_env = VecFrameStack(eval_env, n_stack=4)\n",
    "\n",
    "# The 'evaluate_policy()' function returns a tuple,\n",
    "#       (mean_reward, std_reward)\n",
    "# - 'mean_reward' - refers to the mean reward throughout the episodes\n",
    "# - 'std_reward' - refers to the standard deviation of the reward throughout the episodes\n",
    "print(evaluate_policy(reloaded_A2C_DRL_model, eval_env, n_eval_episodes=1, render=True))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba56faf",
   "metadata": {},
   "source": [
    "## 7. Test the A2C DRL model in a RL Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a7aa91",
   "metadata": {},
   "source": [
    "To test the A2C DRL model in the Gymnasium's 'Breakout-v0' RL Environment, we can use the same code from the earlier section '2. Load RL Environment and testing if it works with a baseline algorithm that takes random actions' with some minor changes\n",
    "\n",
    "But here, instead of taking a random action at each time step in an episode, we are using the A2C DRL model to predict that action at each time step in an episode instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0916eeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.atari_wrappers import AtariWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "14cfdc5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 Score: 0.0\n",
      "Episode: 2 Score: 0.0\n",
      "Episode: 3 Score: 2.0\n",
      "Episode: 4 Score: 0.0\n",
      "Episode: 5 Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "train_env = make_atari_env('Breakout-v0', n_envs=1, seed=0)\n",
    "train_env = VecFrameStack(train_env, n_stack=4)\n",
    "\n",
    "render_env = gym.make('Breakout-v0', render_mode='human')\n",
    "render_env = AtariWrapper(render_env)\n",
    "\n",
    "episodes = 5\n",
    "for episode in range(1, episodes+1):\n",
    "    train_obs = train_env.reset()\n",
    "    render_obs = render_env.reset()\n",
    "    done = False\n",
    "    episode_score = 0\n",
    "\n",
    "    while not done:\n",
    "        render_env.render()\n",
    "        \n",
    "        action, _ = reloaded_A2C_DRL_model.predict(train_obs)\n",
    "        \n",
    "        train_obs, reward, dones, infos = train_env.step(action)\n",
    "        render_obs, reward, done, truncated, info = render_env.step(action[0])  \n",
    "        \n",
    "        episode_score += reward\n",
    "\n",
    "    print(f\"Episode: {episode} Score: {episode_score}\")\n",
    "\n",
    "# Close environments\n",
    "train_env.close()\n",
    "render_env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
