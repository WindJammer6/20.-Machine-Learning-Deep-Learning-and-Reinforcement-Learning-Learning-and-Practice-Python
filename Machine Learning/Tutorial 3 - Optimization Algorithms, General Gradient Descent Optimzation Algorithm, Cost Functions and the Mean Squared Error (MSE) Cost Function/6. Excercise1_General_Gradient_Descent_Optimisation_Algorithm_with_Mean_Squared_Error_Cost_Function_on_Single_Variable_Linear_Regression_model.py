# Question 1:
# You are given the mathematics and computer science courses test scores by some students. You are
# tasked to find the correlation between the 2 courses's test scores by these students using Single
# Variable Linear Regression (SVLR) ML algorithm in these sub-questions.

# a. For the test scores in the 'test_scores.csv' file, run the (general) Gradient Descent Optimization 
#    Algorithm, and stopping the Gradient Desent Optimization Algorithm when the difference in value 
#    of the Cost Function between iterations becomes smaller than 1 x 10^-20. State, the value of the:
#    - weights ('m')
#    - biases ('b')
#    - Cost Function
#    - appropriate learning rate
#    - number of iterations required 

# b. Now use the Scikit-learn's 'sklearn.linear_model' find the coefficient (i.e. weight ('m')) and 
#    intercept (i.e. biases ('b')). Compare them with the weights ('m'), biases ('b') generated by the
#    (general) Gradient Desent Optimization Algorithm.



import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression

# What this function does is that it implements the (general) Gradient Descent Optimization Algorithm with the
# Mean Squared Error (MSE) Cost Function, in context of the Single Variable Linear Regression (SVLR) ML algorithm.
def general_gradient_descent_optimization_algorithm(x, y):

    current_weight_m_value = 0
    current_biases_b_value = 0


    # a. For the SVLR ML model trained using the 'test_scores.csv' dataset with the (general) Gradient
    #    Descent Optimization Algorithm with the Mean Squared Error (MSE) Cost Function, and stopping 
    #    the Gradient Desent Optimization Algorithm when the difference in value of the Cost Function 
    #    between iterations becomes smaller than 1 x 10^-20, the values are:
    #    - weights ('m') - 1.017738298061554
    #    - biases ('b') - 1.9150733098031267
    #    - Cost Function - 31.604511334753223
    #    - appropriate learning rate - 0.0002 (obtained through trial and error as explained by the 
    #                                          'general method' in the file: '3. changing_
    #                                          the_learning_rate_value_and_number_of_iterations_value_
    #                                          to_find_the_set_of_weights_and_biases_with_the_minimal_
    #                                          Cost_Function_value_in_the_General_Gradient_Descent_
    #                                          Optimization_Algorithm.py')
    #    - number of iterations required - 412665
    number_of_iterations = 1000000
    n = len(x)
    learning_rate_value = 0.0002

    for i in range(number_of_iterations):

        y_predicted = current_weight_m_value * x + current_biases_b_value

        current_cost_function_value = (1/n) * sum(((y - (current_weight_m_value * x + current_biases_b_value)))**2)
        # current_cost_function_value = (1/n) * sum(((y - y_predicted))**2) works too

        partial_derivative_of_weight_m_with_respect_to_Cost_Function = -(2/n) * sum(x * (y - (current_weight_m_value * x + current_biases_b_value)))
        # partial_derivative_of_weight_m_with_respect_to_Cost_Function = -(2/n) * sum(x*(y - y_predicted)) works too 
        partial_derivative_of_biases_b_with_respect_to_Cost_Function = -(2/n) * sum((y - (current_weight_m_value * x + current_biases_b_value)))
        # partial_derivative_of_biases_b_with_respect_to_Cost_Function = -(2/n) * sum(x*(y - y_predicted)) works too

        print("Current iteration: {}, Current weight ('m') value: {}, Current biases ('b') value: {}, Cost Function value: {}".format(i, current_weight_m_value, current_biases_b_value, current_cost_function_value))
        
        current_weight_m_value = current_weight_m_value - (learning_rate_value * partial_derivative_of_weight_m_with_respect_to_Cost_Function)
        current_biases_b_value = current_biases_b_value - (learning_rate_value * partial_derivative_of_biases_b_with_respect_to_Cost_Function)

        if current_cost_function_value - ((1/n) * sum(((y - (current_weight_m_value * x + current_biases_b_value)))**2)) < 10**(-20):
            break
        else:
            pass 


if __name__ == '__main__':
    dataset = pd.read_csv('test_scores.csv')
    print(dataset)
    
    x = np.array(dataset['math'])
    y = np.array(dataset['computer_science'])

    general_gradient_descent_optimization_algorithm(x, y)


    # b. For the SVLR ML model trained using the 'test_scores.csv' dataset with Scikit-learn's 
    #    'sklearn.linear_model' to find the coefficient (i.e. weight ('m')) and intercept 
    #    (i.e. biases ('b')). The values are:
    #    - coefficient (i.e. weight ('m')) - 1.01773624
    #    - intercept (i.e. biases ('b')) - 1.91521931

    #    Comparing them with the weights ('m') and biases ('b') generated by the (general) Gradient 
    #    Desent Optimization Algorithm:
    #    The values of the weights ('m') and biases ('b') obtained from the (general) Gradient
    #    Descent Optimization Algorithm with the Mean Squared Error (MSE) Cost Function, and stopping 
    #    the Gradient Desent Optimization Algorithm when the difference in value of the Cost Function 
    #    between iterations becomes smaller than 1 x 10^-20,
    #    - weights ('m') - 1.017738298061554
    #    - biases ('b') - 1.9150733098031267

    #    The values of the weights ('m') and biases ('b') obtained from the SVLR ML model trained 
    #    using the 'test_scores.csv' dataset with Scikit-learn's 'sklearn.linear_model',
    #    - weights ('m') (i.e. coefficient) - 1.01773624
    #    - biases ('b') (i.e. intercept) - 1.91521931
    single_variable_linear_regression_model = LinearRegression()
    single_variable_linear_regression_model.fit(dataset[['math']], dataset[['computer_science']])    # (i.e. (general) Graident Descent 
                                                                                                            # Optimization Algorithm with Mean 
                                                                                                            # Squared Error (MSE) Cost Function)
    print(single_variable_linear_regression_model)                                                                                                        
    print(single_variable_linear_regression_model.coef_)        # (i.e. weight ('m'))
    print(single_variable_linear_regression_model.intercept_)   #  (i.e. biases ('b'))




# My answers are all correct.